{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AnalyseModel.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPuYFuPqV+9L+MwdqF3hotY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobatgithub/unreverb/blob/main/AnalyseModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ygE0WiSBwZrd"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow_io\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "path = '/content/drive/My Drive/dsr_project/data/HarvardWordList/'\n",
        "save_path = path + 'datasets/'\n",
        "\n",
        "ir_base_path = '/content/drive/My Drive/dsr_project/data/samplicity-ir/'\n",
        "irpaths0 = glob.glob(ir_base_path + '*/*M-to-S.wav')\n",
        "irpaths0 = [irpath.split('/')[-1] for irpath in irpaths0]\n",
        "#irpaths1 = glob.glob(save_path + 'IR_[0-5]*.wav')\n",
        "#irpaths1 = [irpath.split('/')[-1] for irpath in irpaths1]\n",
        "#irpaths1 = [irpath.split('_')[2:][0] for irpath in irpaths1]\n",
        "#print(irpaths0)\n",
        "#print(irpaths1)\n",
        "\n",
        "irpaths = []\n",
        "for i in range(len(irpaths0)):\n",
        "  irpaths.append(save_path + 'IR_' +str(i) + '_' + irpaths0[i])\n",
        "\n",
        "irpaths = irpaths[0:6]\n",
        "irpaths"
      ],
      "metadata": {
        "id": "Tn5hwAB_1phZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os, shutil\n",
        "\n",
        "sample_rate = 2**14\n",
        "\n",
        "# load data\n",
        "#path = '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/'\n",
        "#save_path = path + 'datasets_M/'\n",
        "path = '/content/drive/My Drive/dsr_project/data/HarvardWordList/'\n",
        "save_path = path + 'datasets/'\n",
        "\n",
        "ir_length = len(irpaths)\n",
        "\n",
        "dataset_filenames = glob.glob(save_path + 'tf_IR_*')\n",
        "dataset_filenames = dataset_filenames[:ir_length]\n",
        "\n",
        "datasets = []\n",
        "for filename in dataset_filenames:\n",
        "  # load the tensorflow dataset\n",
        "  datasets.append(tf.data.experimental.load(filename))\n",
        "\n",
        "dataset = datasets[0]\n",
        "for i in range(1, len(datasets)):\n",
        "#for i in range(1, len(irpaths)):\n",
        "  dataset = dataset.concatenate(datasets[i])\n",
        "\n",
        "# determine size of the dataset\n",
        "dataset_size = sum(1 for _ in dataset)\n",
        "print(f\"dataset_size: {dataset_size}\")"
      ],
      "metadata": {
        "id": "bgMzP2r7wjs4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lambda_1(features, labels, targets):\n",
        "  return (features, labels)\n",
        "\n",
        "def lambda_2(features, labels, targets):\n",
        "  return (features, targets)\n",
        "\n",
        "def lambda_3(features, labels, targets):\n",
        "  return labels\n",
        "\n",
        "# obtain the datasets containing only labels or targets\n",
        "label_dataset = dataset.map(lambda features, labels, targets: lambda_1(features, labels, targets))\n",
        "target_dataset = dataset.map(lambda features, labels, targets: lambda_2(features, labels, targets))\n",
        "\n",
        "# obtain all labels\n",
        "all_labels = dataset.map(lambda features, labels, targets: lambda_3(features, labels, targets))\n",
        "\n",
        "# transform all_labels dataset to numpy array\n",
        "all_labels_np = []\n",
        "for label in all_labels:\n",
        "  all_labels_np.append(tfds.as_numpy(label))\n",
        "\n",
        "all_labels_np = np.array(all_labels_np)\n",
        "\n",
        "# determine unique labels\n",
        "unique_labels, counts = np.unique(all_labels_np, return_counts=True)\n",
        "print(f\"unique_labels: {unique_labels}\")\n",
        "#print(counts)\n",
        "\n",
        "# determine number of unique labels\n",
        "nr_of_unique_labels = unique_labels.shape[0]\n",
        "print(f\"nr_of_unique_labels: {nr_of_unique_labels}\")\n",
        "\n",
        "#dataset = dataset.shuffle(dataset_size)\n",
        "\n",
        "train_fraction = 0.9\n",
        "validate_dataset_size = int(dataset_size * (1.0-train_fraction)) # 20 percent of dataset_size\n",
        "#total_dataset = dataset.take(validate_dataset_size)\n",
        "total_dataset = dataset"
      ],
      "metadata": {
        "id": "17Qfast3wl7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_io as tfio\n",
        "\n",
        "def preprocess(sample, target):\n",
        "  audio = tf.cast(sample, tf.float32)\n",
        "  label = target\n",
        "  #print(audio)\n",
        "  #print(label)\n",
        "  spectrogram = tfio.audio.spectrogram(audio, nfft=1024, window=1024, stride=64)\n",
        "  spectrogram = tfio.audio.melscale(spectrogram, rate=sample_rate, mels=128, fmin=0, fmax=2000)\n",
        "  spectrogram /= tf.math.reduce_max(spectrogram)\n",
        "  spectrogram = tf.expand_dims(spectrogram, axis=-1)\n",
        "\n",
        "  spectrogram = tf.image.resize(spectrogram, (128, 128))\n",
        "  spectrogram = tf.transpose(spectrogram, perm=(1, 0, 2))\n",
        "  spectrogram = spectrogram[::-1, :, :]\n",
        "\n",
        "  return spectrogram, label\n",
        "\n",
        "dataset_total = total_dataset.map(lambda feature, label, target: preprocess(feature, label))\n",
        "dataset_total = dataset_total.cache()\n",
        "dataset_total = dataset_total.batch(32)"
      ],
      "metadata": {
        "id": "_vkACsSPw14f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/' + 'datasets_F/'\n",
        "model = tf.keras.models.load_model(model_save_path + 'my_model_2.h5')\n",
        "model.summary()\n",
        "model.evaluate(dataset_total)"
      ],
      "metadata": {
        "id": "gfoOBKsuxCw9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_total = []\n",
        "y_total = []\n",
        "for sample, label in dataset_total:\n",
        "  X_total.append(sample.numpy())\n",
        "  y_total.append(label.numpy())\n",
        "\n",
        "probabilities = []\n",
        "for X in X_total:\n",
        "  probabilities.extend(model.predict(X))\n",
        "\n",
        "probabilities = np.asarray(probabilities)  "
      ],
      "metadata": {
        "id": "le68uW6cxJO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "max_probabilities = []\n",
        "for probs in probabilities:\n",
        "  max_probabilities.append(np.max(probs))\n",
        "\n",
        "min_max_probabilities = np.min(max_probabilities)\n",
        "print(min_max_probabilities)\n",
        "binned_probs, bins, patches = plt.hist(max_probabilities, bins=10)\n",
        "plt.show()\n",
        "print(binned_probs)\n",
        "print(bins)"
      ],
      "metadata": {
        "id": "rdZ0Y9PsxO1K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_total = []\n",
        "targets_total = []\n",
        "lables_total = []\n",
        "for sample, lable, target in total_dataset:\n",
        "  samples_total.append(sample.numpy())\n",
        "  targets_total.append(target.numpy())\n",
        "  lables_total.append(lable.numpy())\n",
        "\n",
        "samples_total = np.asarray(samples_total)\n",
        "targets_total = np.asarray(targets_total)\n",
        "lables_total = np.asarray(lables_total)\n",
        "\n",
        "print(samples_total.shape)\n",
        "print(targets_total.shape)\n",
        "\n",
        "unique_elements, counts_elements = np.unique(lables_total, return_counts=True)\n",
        "print(unique_elements)\n",
        "print(counts_elements)"
      ],
      "metadata": {
        "id": "IFnjBLhdxTbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.fft import fft, ifft\n",
        "import librosa\n",
        "\n",
        "#irpaths = glob.glob(save_path + 'IR_*.wav')\n",
        "\n",
        "def wiener_deconvolution(signal, kernel, lambd=1e-3):\n",
        "  kernel = np.hstack((kernel, np.zeros(len(signal) - len(kernel)))) # zero pad the kernel to same length\n",
        "  H = fft(kernel)\n",
        "  Y = fft(signal)\n",
        "  #S = np.abs(fft(signal))**2\n",
        "  #GY = Y*np.conj(H)*S/(H*np.conj(H)*S + lambd**2)\n",
        "  GY = Y*np.conj(H)/(H*np.conj(H) + lambd**2)\n",
        "  deconvolved = np.real(ifft(GY))\n",
        "  return deconvolved\n",
        "\n",
        "def my_conv(signal, kernel):\n",
        "  kernel_padded = np.hstack((kernel, np.zeros(len(signal) - len(kernel))))\n",
        "  convolved_signal = np.real(ifft(fft(kernel_padded)*fft(signal)))\n",
        "  return convolved_signal\n",
        "\n",
        "irs = []\n",
        "#for irpath in irpaths:\n",
        "for i in range(len(irpaths)):\n",
        "  ir, IR_sample_rate = librosa.load(irpaths[i], sr=sample_rate)\n",
        "  irs.append(ir)\n",
        "\n",
        "all_lengths = []\n",
        "for ir in irs:\n",
        "  all_lengths.append(len(ir))\n",
        "\n",
        "max_length = max(all_lengths)    \n",
        "min_length = min(all_lengths)\n",
        "print(max_length/sample_rate)\n",
        "print(min_length/sample_rate)"
      ],
      "metadata": {
        "id": "4DaJrgYXxnCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "i00 = 3*2*301\n",
        "i0, i1 = i00 + 0, i00 + 10\n",
        "test_target = np.concatenate(targets_total[i0:i1])\n",
        "test_sample = np.concatenate(samples_total[i0:i1])\n",
        "test_probabilities = probabilities[i0:i1]\n",
        "mean_test_probabilities = np.mean(test_probabilities, axis = 0)\n",
        "print(f\"mean_test_probabilities: {mean_test_probabilities}\")\n",
        "most_probable_ir = np.argmax(mean_test_probabilities)\n",
        "print(f\"most_probable_ir: {most_probable_ir}\")\n",
        "test_deconv = wiener_deconvolution(test_sample, irs[most_probable_ir])\n",
        "display(Audio(test_target, rate=sample_rate))\n",
        "display(Audio(test_sample, rate=sample_rate))\n",
        "display(Audio(test_deconv, rate=sample_rate))"
      ],
      "metadata": {
        "id": "J67nnyHbsicB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities.shape"
      ],
      "metadata": {
        "id": "EnvNXAlH4Wrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probabilities2 = probabilities.reshape((6,-1,6))\n",
        "probabilities2.shape"
      ],
      "metadata": {
        "id": "HN8_bMHa4ZZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets_total.shape"
      ],
      "metadata": {
        "id": "MfWnpr-_7iBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "PJcgP6sC8j-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BrOPf5Yd7ixu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}