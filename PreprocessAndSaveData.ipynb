{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreprocessAndSaveData.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobatgithub/unreverb/blob/main/PreprocessAndSaveData.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_20e4BH03sR",
        "outputId": "f273093d-0f25-4177-fe86-fc839bd63afe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (1.21.20)\n",
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.7/dist-packages (0.24.0)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio) (3.10.0.2)\n",
            "Requirement already satisfied: soundfile>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.10.3.post1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.21.5)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.6.0)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.3)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (3.0.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.25.11)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn!=0.19.0,>=0.14.0->librosa) (3.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile>=0.10.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.21)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.10.0)\n",
            "Requirement already satisfied: botocore<1.25.0,>=1.24.20 in /usr/local/lib/python3.7/dist-packages (from boto3) (1.24.20)\n",
            "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3) (0.5.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.25.0,>=1.24.20->boto3) (2.8.2)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.24.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.24.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.7/dist-packages (0.25.1)\n",
            "Requirement already satisfied: tensorflow_io in /usr/local/lib/python3.7/dist-packages (0.24.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem==0.24.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_io) (0.24.0)\n",
            "1.10.0+cu111\n",
            "0.10.0+cu111\n"
          ]
        }
      ],
      "source": [
        "#@title Imports\n",
        "# When running this tutorial in Google Colab, install the required packages\n",
        "# with the following.\n",
        "!pip install torchaudio librosa boto3 tensorflow_io\n",
        "!pip install pydub tensorflow_io\n",
        "\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import tensorflow as tf\n",
        "import torchaudio.transforms as T\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_io as tfio\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "print(torch.__version__)\n",
        "print(torchaudio.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FlbhZHXidBAH",
        "outputId": "d1a0af34-8c38-4db9-c340-e92bb675a26f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import walk\n",
        "\n",
        "mypath = \"/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02\"\n",
        "f = []\n",
        "for (dirpath, dirnames, filenames) in walk(mypath):\n",
        "    f.extend(filenames)\n",
        "    break\n",
        "\n",
        "print(dirnames)\n",
        "print(filenames)\n",
        "print(mypath + filenames[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zpzdk0emBqvA",
        "outputId": "d9d69213-8660-47a4-a640-8eddcae18ceb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "['F-22-02-023.wav', 'F-22-02-010.wav', 'F-22-02-019.wav', 'F-22-02-005.wav', 'F-22-02-028.wav', 'F-22-02-013.wav', 'F-22-02-022.wav', 'F-22-02-003.wav', 'F-22-02-011.wav', 'F-22-02-020.wav', 'F-22-02-004.wav', 'F-22-02-032.wav', 'F-22-02-014.wav', 'F-22-02-021.wav', 'F-22-02-017.wav', 'F-22-02-030.wav', 'F-22-02-007.wav', 'F-22-02-018.wav', 'F-22-02-006.wav', 'F-22-02-027.wav', 'F-22-02-025.wav', 'F-22-02-024.wav']\n",
            "/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02F-22-02-023.wav\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import os\n",
        "import math\n",
        "import tarfile\n",
        "import multiprocessing\n",
        "\n",
        "import scipy\n",
        "import librosa\n",
        "# import boto3\n",
        "from botocore import UNSIGNED\n",
        "# from botocore.config import Config\n",
        "import requests\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import time\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "[width, height] = matplotlib.rcParams['figure.figsize']\n",
        "if width < 10:\n",
        "  matplotlib.rcParams['figure.figsize'] = [width * 2.5, height]\n",
        "\n",
        "_SAMPLE_DIR = \"_sample_data\"\n",
        "SAMPLE_WAV_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.wav\"\n",
        "SAMPLE_WAV_PATH = os.path.join(_SAMPLE_DIR, \"steam.wav\")\n",
        "\n",
        "SAMPLE_WAV_SPEECH_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
        "SAMPLE_WAV_SPEECH_PATH = os.path.join(_SAMPLE_DIR, \"speech.wav\")\n",
        "\n",
        "SAMPLE_RIR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/room-response/rm1/impulse/Lab41-SRI-VOiCES-rm1-impulse-mc01-stu-clo.wav\"\n",
        "SAMPLE_RIR_PATH = os.path.join(_SAMPLE_DIR, \"rir.wav\")\n",
        "\n",
        "SAMPLE_NOISE_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit/distant-16k/distractors/rm1/babb/Lab41-SRI-VOiCES-rm1-babb-mc01-stu-clo.wav\"\n",
        "SAMPLE_NOISE_PATH = os.path.join(_SAMPLE_DIR, \"bg.wav\")\n",
        "\n",
        "SAMPLE_MP3_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.mp3\"\n",
        "SAMPLE_MP3_PATH = os.path.join(_SAMPLE_DIR, \"steam.mp3\")\n",
        "\n",
        "SAMPLE_GSM_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/steam-train-whistle-daniel_simon.gsm\"\n",
        "SAMPLE_GSM_PATH = os.path.join(_SAMPLE_DIR, \"steam.gsm\")\n",
        "\n",
        "SAMPLE_TAR_URL = \"https://pytorch-tutorial-assets.s3.amazonaws.com/VOiCES_devkit.tar.gz\"\n",
        "SAMPLE_TAR_PATH = os.path.join(_SAMPLE_DIR, \"sample.tar.gz\")\n",
        "SAMPLE_TAR_ITEM = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\"\n",
        "\n",
        "S3_BUCKET = \"pytorch-tutorial-assets\"\n",
        "S3_KEY = \"VOiCES_devkit/source-16k/train/sp0307/Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav\""
      ],
      "metadata": {
        "id": "c-ULQS_xChhJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# YESNO_DATASET_PATH = os.path.join(_SAMPLE_DIR, \"yes_no\")\n",
        "# os.makedirs(YESNO_DATASET_PATH, exist_ok=True)\n",
        "# os.makedirs(_SAMPLE_DIR, exist_ok=True)\n",
        "\n",
        "def _fetch_data():\n",
        "  uri = [\n",
        "    (SAMPLE_WAV_URL, SAMPLE_WAV_PATH),\n",
        "    (SAMPLE_WAV_SPEECH_URL, SAMPLE_WAV_SPEECH_PATH),\n",
        "    (SAMPLE_RIR_URL, SAMPLE_RIR_PATH),\n",
        "    (SAMPLE_NOISE_URL, SAMPLE_NOISE_PATH),\n",
        "    (SAMPLE_MP3_URL, SAMPLE_MP3_PATH),\n",
        "    (SAMPLE_GSM_URL, SAMPLE_GSM_PATH),\n",
        "    (SAMPLE_TAR_URL, SAMPLE_TAR_PATH),\n",
        "  ]\n",
        "#   for url, path in uri:\n",
        "#     with open(path, 'wb') as file_:\n",
        "#       file_.write(requests.get(url).content)\n",
        "\n",
        "# _fetch_data()\n",
        "\n",
        "def _download_yesno():\n",
        "  if os.path.exists(os.path.join(YESNO_DATASET_PATH, \"waves_yesno.tar.gz\")):\n",
        "    return\n",
        "  torchaudio.datasets.YESNO(root=YESNO_DATASET_PATH, download=True)\n",
        "\n",
        "# YESNO_DOWNLOAD_PROCESS = multiprocessing.Process(target=_download_yesno)\n",
        "# YESNO_DOWNLOAD_PROCESS.start()\n",
        "\n",
        "def _get_sample(path, resample=None):\n",
        "  effects = [\n",
        "    [\"remix\", \"1\"]\n",
        "  ]\n",
        "  if resample:\n",
        "    effects.extend([\n",
        "      [\"lowpass\", f\"{resample // 2}\"],\n",
        "      [\"rate\", f'{resample}'],\n",
        "    ])\n",
        "  return torchaudio.sox_effects.apply_effects_file(path, effects=effects)\n",
        "\n",
        "def get_speech_sample(*, resample=None):\n",
        "  return _get_sample(SAMPLE_WAV_SPEECH_PATH, resample=resample)\n",
        "\n",
        "def get_sample(*, resample=None):\n",
        "  return _get_sample(SAMPLE_WAV_PATH, resample=resample)\n",
        "\n",
        "def my_get_sample(audiopath, resample=None):\n",
        "  return _get_sample(audiopath, resample=resample)\n",
        "\n",
        "def get_rir_sample(*, resample=None, processed=False):\n",
        "  ir_raw, sample_rate = _get_sample(SAMPLE_RIR_PATH, resample=resample)\n",
        "  if not processed:\n",
        "    return ir_raw, sample_rate\n",
        "  rir = ir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
        "  rir = rir / torch.norm(rir, p=2)\n",
        "  rir = torch.flip(rir, [1])\n",
        "  return rir, sample_rate\n",
        "\n",
        "def my_get_ir_sample(irpath, resample=None, processed=False):\n",
        "  ir_raw, sample_rate = _get_sample(irpath, resample=resample)\n",
        "  if not processed:\n",
        "    return ir_raw, sample_rate\n",
        "  ir = ir_raw[:, int(sample_rate*1.01):int(sample_rate*1.3)]\n",
        "  ir = ir / torch.norm(ir, p=2)\n",
        "  ir = torch.flip(ir, [1])\n",
        "  return ir, sample_rate\n",
        "\n",
        "def get_noise_sample(*, resample=None):\n",
        "  return _get_sample(SAMPLE_NOISE_PATH, resample=resample)\n",
        "\n",
        "def print_stats(waveform, sample_rate=None, src=None):\n",
        "  if src:\n",
        "    print(\"-\" * 10)\n",
        "    print(\"Source:\", src)\n",
        "    print(\"-\" * 10)\n",
        "  if sample_rate:\n",
        "    print(\"Sample Rate:\", sample_rate)\n",
        "  print(\"Shape:\", tuple(waveform.shape))\n",
        "  print(\"Dtype:\", waveform.dtype)\n",
        "  print(f\" - Max:     {waveform.max().item():6.3f}\")\n",
        "  print(f\" - Min:     {waveform.min().item():6.3f}\")\n",
        "  print(f\" - Mean:    {waveform.mean().item():6.3f}\")\n",
        "  print(f\" - Std Dev: {waveform.std().item():6.3f}\")\n",
        "  print()\n",
        "  print(waveform)\n",
        "  print()\n",
        "\n",
        "def plot_waveform(waveform, sample_rate, title=\"Waveform\", xlim=None, ylim=None):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    axes[c].plot(time_axis, waveform[c], linewidth=1)\n",
        "    axes[c].grid(True)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "    if ylim:\n",
        "      axes[c].set_ylim(ylim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def plot_specgram(waveform, sample_rate, title=\"Spectrogram\", xlim=None):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  time_axis = torch.arange(0, num_frames) / sample_rate\n",
        "\n",
        "  figure, axes = plt.subplots(num_channels, 1)\n",
        "  if num_channels == 1:\n",
        "    axes = [axes]\n",
        "  for c in range(num_channels):\n",
        "    axes[c].specgram(waveform[c], Fs=sample_rate)\n",
        "    if num_channels > 1:\n",
        "      axes[c].set_ylabel(f'Channel {c+1}')\n",
        "    if xlim:\n",
        "      axes[c].set_xlim(xlim)\n",
        "  figure.suptitle(title)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def play_audio(waveform, sample_rate):\n",
        "  waveform = waveform.numpy()\n",
        "\n",
        "  num_channels, num_frames = waveform.shape\n",
        "  if num_channels == 1:\n",
        "    display(Audio(waveform[0], rate=sample_rate))\n",
        "  elif num_channels == 2:\n",
        "    display(Audio((waveform[0], waveform[1]), rate=sample_rate))\n",
        "  else:\n",
        "    raise ValueError(\"Waveform with more than 2 channels are not supported.\")\n",
        "\n",
        "def inspect_file(path):\n",
        "  print(\"-\" * 10)\n",
        "  print(\"Source:\", path)\n",
        "  print(\"-\" * 10)\n",
        "  print(f\" - File size: {os.path.getsize(path)} bytes\")\n",
        "  print(f\" - {torchaudio.info(path)}\")\n",
        "\n",
        "def plot_spectrogram(spec, title=None, ylabel='freq_bin', aspect='auto', xmax=None):\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or 'Spectrogram (db)')\n",
        "  axs.set_ylabel(ylabel)\n",
        "  axs.set_xlabel('frame')\n",
        "  im = axs.imshow(librosa.power_to_db(spec), origin='lower', aspect=aspect)\n",
        "  if xmax:\n",
        "    axs.set_xlim((0, xmax))\n",
        "  fig.colorbar(im, ax=axs)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def plot_mel_fbank(fbank, title=None):\n",
        "  fig, axs = plt.subplots(1, 1)\n",
        "  axs.set_title(title or 'Filter bank')\n",
        "  axs.imshow(fbank, aspect='auto')\n",
        "  axs.set_ylabel('frequency bin')\n",
        "  axs.set_xlabel('mel bin')\n",
        "  plt.show(block=False)\n",
        "\n",
        "def get_spectrogram(\n",
        "    n_fft = 400,\n",
        "    win_len = None,\n",
        "    hop_len = None,\n",
        "    power = 2.0,\n",
        "):\n",
        "  waveform, _ = get_speech_sample()\n",
        "  spectrogram = T.Spectrogram(\n",
        "      n_fft=n_fft,\n",
        "      win_length=win_len,\n",
        "      hop_length=hop_len,\n",
        "      center=True,\n",
        "      pad_mode=\"reflect\",\n",
        "      power=power,\n",
        "  )\n",
        "  return spectrogram(waveform)\n",
        "\n",
        "def plot_pitch(waveform, sample_rate, pitch):\n",
        "  figure, axis = plt.subplots(1, 1)\n",
        "  axis.set_title(\"Pitch Feature\")\n",
        "  axis.grid(True)\n",
        "\n",
        "  end_time = waveform.shape[1] / sample_rate\n",
        "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
        "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
        "\n",
        "  axis2 = axis.twinx()\n",
        "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
        "  ln2 = axis2.plot(\n",
        "      time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
        "\n",
        "  axis2.legend(loc=0)\n",
        "  plt.show(block=False)\n",
        "\n",
        "def plot_kaldi_pitch(waveform, sample_rate, pitch, nfcc):\n",
        "  figure, axis = plt.subplots(1, 1)\n",
        "  axis.set_title(\"Kaldi Pitch Feature\")\n",
        "  axis.grid(True)\n",
        "\n",
        "  end_time = waveform.shape[1] / sample_rate\n",
        "  time_axis = torch.linspace(0, end_time,  waveform.shape[1])\n",
        "  axis.plot(time_axis, waveform[0], linewidth=1, color='gray', alpha=0.3)\n",
        "\n",
        "  time_axis = torch.linspace(0, end_time, pitch.shape[1])\n",
        "  ln1 = axis.plot(time_axis, pitch[0], linewidth=2, label='Pitch', color='green')\n",
        "  axis.set_ylim((-1.3, 1.3))\n",
        "\n",
        "  axis2 = axis.twinx()\n",
        "  time_axis = torch.linspace(0, end_time, nfcc.shape[1])\n",
        "  ln2 = axis2.plot(\n",
        "      time_axis, nfcc[0], linewidth=2, label='NFCC', color='blue', linestyle='--')\n",
        "\n",
        "  lns = ln1 + ln2\n",
        "  labels = [l.get_label() for l in lns]\n",
        "  axis.legend(lns, labels, loc=0)\n",
        "  plt.show(block=False)\n",
        "\n",
        "DEFAULT_OFFSET = 201\n",
        "SWEEP_MAX_SAMPLE_RATE = 48000\n",
        "DEFAULT_LOWPASS_FILTER_WIDTH = 6\n",
        "DEFAULT_ROLLOFF = 0.99\n",
        "DEFAULT_RESAMPLING_METHOD = 'sinc_interpolation'\n",
        "\n",
        "def _get_log_freq(sample_rate, max_sweep_rate, offset):\n",
        "  \"\"\"Get freqs evenly spaced out in log-scale, between [0, max_sweep_rate // 2]\n",
        "\n",
        "  offset is used to avoid negative infinity `log(offset + x)`.\n",
        "\n",
        "  \"\"\"\n",
        "  half = sample_rate // 2\n",
        "  start, stop = math.log(offset), math.log(offset + max_sweep_rate // 2)\n",
        "  return torch.exp(torch.linspace(start, stop, sample_rate, dtype=torch.double)) - offset\n",
        "\n",
        "def _get_inverse_log_freq(freq, sample_rate, offset):\n",
        "  \"\"\"Find the time where the given frequency is given by _get_log_freq\"\"\"\n",
        "  half = sample_rate // 2\n",
        "  return sample_rate * (math.log(1 + freq / offset) / math.log(1 + half / offset))\n",
        "\n",
        "def _get_freq_ticks(sample_rate, offset, f_max):\n",
        "  # Given the original sample rate used for generating the sweep,\n",
        "  # find the x-axis value where the log-scale major frequency values fall in\n",
        "  time, freq = [], []\n",
        "  for exp in range(2, 5):\n",
        "    for v in range(1, 10):\n",
        "      f = v * 10 ** exp\n",
        "      if f < sample_rate // 2:\n",
        "        t = _get_inverse_log_freq(f, sample_rate, offset) / sample_rate\n",
        "        time.append(t)\n",
        "        freq.append(f)\n",
        "  t_max = _get_inverse_log_freq(f_max, sample_rate, offset) / sample_rate\n",
        "  time.append(t_max)\n",
        "  freq.append(f_max)\n",
        "  return time, freq\n",
        "\n",
        "def plot_sweep(waveform, sample_rate, title, max_sweep_rate=SWEEP_MAX_SAMPLE_RATE, offset=DEFAULT_OFFSET):\n",
        "  x_ticks = [100, 500, 1000, 5000, 10000, 20000, max_sweep_rate // 2]\n",
        "  y_ticks = [1000, 5000, 10000, 20000, sample_rate//2]\n",
        "\n",
        "  time, freq = _get_freq_ticks(max_sweep_rate, offset, sample_rate // 2)\n",
        "  freq_x = [f if f in x_ticks and f <= max_sweep_rate // 2 else None for f in freq]\n",
        "  freq_y = [f for f in freq if f >= 1000 and f in y_ticks and f <= sample_rate // 2]\n",
        "\n",
        "  figure, axis = plt.subplots(1, 1)\n",
        "  axis.specgram(waveform[0].numpy(), Fs=sample_rate)\n",
        "  plt.xticks(time, freq_x)\n",
        "  plt.yticks(freq_y, freq_y)\n",
        "  axis.set_xlabel('Original Signal Frequency (Hz, log scale)')\n",
        "  axis.set_ylabel('Waveform Frequency (Hz)')\n",
        "  axis.xaxis.grid(True, alpha=0.67)\n",
        "  axis.yaxis.grid(True, alpha=0.67)\n",
        "  figure.suptitle(f'{title} (sample rate: {sample_rate} Hz)')\n",
        "  plt.show(block=True)\n",
        "\n",
        "def get_sine_sweep(sample_rate, offset=DEFAULT_OFFSET):\n",
        "    max_sweep_rate = sample_rate\n",
        "    freq = _get_log_freq(sample_rate, max_sweep_rate, offset)\n",
        "    delta = 2 * math.pi * freq / sample_rate\n",
        "    cummulative = torch.cumsum(delta, dim=0)\n",
        "    signal = torch.sin(cummulative).unsqueeze(dim=0)\n",
        "    return signal\n",
        "\n",
        "def benchmark_resample(\n",
        "    method,\n",
        "    waveform,\n",
        "    sample_rate,\n",
        "    resample_rate,\n",
        "    lowpass_filter_width=DEFAULT_LOWPASS_FILTER_WIDTH,\n",
        "    rolloff=DEFAULT_ROLLOFF,\n",
        "    resampling_method=DEFAULT_RESAMPLING_METHOD,\n",
        "    beta=None,\n",
        "    librosa_type=None,\n",
        "    iters=5\n",
        "):\n",
        "  if method == \"functional\":\n",
        "    begin = time.time()\n",
        "    for _ in range(iters):\n",
        "      F.resample(waveform, sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
        "                 rolloff=rolloff, resampling_method=resampling_method)\n",
        "    elapsed = time.time() - begin\n",
        "    return elapsed / iters\n",
        "  elif method == \"transforms\":\n",
        "    resampler = T.Resample(sample_rate, resample_rate, lowpass_filter_width=lowpass_filter_width,\n",
        "                           rolloff=rolloff, resampling_method=resampling_method, dtype=waveform.dtype)\n",
        "    begin = time.time()\n",
        "    for _ in range(iters):\n",
        "      resampler(waveform)\n",
        "    elapsed = time.time() - begin\n",
        "    return elapsed / iters\n",
        "  elif method == \"librosa\":\n",
        "    waveform_np = waveform.squeeze().numpy()\n",
        "    begin = time.time()\n",
        "    for _ in range(iters):\n",
        "      librosa.resample(waveform_np, sample_rate, resample_rate, res_type=librosa_type)\n",
        "    elapsed = time.time() - begin\n",
        "    return elapsed / iters\n"
      ],
      "metadata": {
        "id": "xdVeIjrfDLk9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls '/content/drive/My Drive/dsr_project/data/HarvardWordList/'\n",
        "#audiopaths = !ls '/content/drive/My Drive/dsr_project/data/Anechoic/'*'/mono/'*'.wav'\n",
        "#audiopaths = [x.replace(\"\\'\", \"\") for x in audiopaths]"
      ],
      "metadata": {
        "id": "VlSL6nfqaIo6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/'\n",
        "audiopaths = !ls '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-2'*'/'*'.wav'\n",
        "audiopaths = [x.replace(\"\\'\", \"\") for x in audiopaths]\n",
        "audiopaths"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JCZXqmQzRIT",
        "outputId": "539312d4-43b0-4961-e7f5-fc9685b8d942"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-003.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-004.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-005.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-006.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-007.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-010.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-011.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-013.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-014.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-017.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-018.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-019.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-020.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-021.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-022.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-023.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-024.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-025.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-027.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-028.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-030.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-22-02/F-22-02-032.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-001.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-004.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-005.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-006.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-007.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-008.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-011.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-012.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-013.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-014.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-015.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-016.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-017.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-018.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-019.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-020.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-021.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-022.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-025.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-028.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-029.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-031.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-033.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-034.wav',\n",
              " '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F-26-18/F-26-18-035.wav']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is done here:\n",
        "#   we load the audio data without reverb - the input\n",
        "#   we load the impulse response (IR) functions\n",
        "#   we generate the audio data with reverb by convolving all impulse response functions with all audio data without reverb\n",
        "#   we transform all data to tensorflow datasets\n",
        "#   one can solve two different tasks:\n",
        "#     classification: which impulse response function is used\n",
        "#     regression: remove the reverb from the input\n",
        "#   correspondingly, we can generate two different datasets here:\n",
        "#     one with the labels (consequtive numbers of the IRs) as the targets for classification\n",
        "#     one with the audios without reverb as the targets (commented out)\n",
        "\n",
        "# sampling rate for resampling\n",
        "sample_rate = 8000\n",
        "\n",
        "# all audio input should have the same length\n",
        "# we split all audio data into chunks of length n\n",
        "# n = 2*sample_rate corresponds to 2 seconds\n",
        "# correspondingly, all audio data without reverb must be at least 2 seconds long\n",
        "n = 8*sample_rate\n",
        "\n",
        "# paths for audio without reverb\n",
        "#audiopaths = [\"/content/drive/My Drive/dsr_project/data/HarvardWordList/SIHarvardWordListsFemale.wav\",\n",
        "#              \"/content/drive/My Drive/dsr_project/data/HarvardWordList/SIHarvardWordListsMale.wav\"]\n",
        "\n",
        "# paths to IR functions\n",
        "irpaths = [\n",
        "           \"/content/drive/My Drive/dsr_project/data/r1-nuclear-reactor-hall/b-format/r1_bformat.wav\", \n",
        "           \"/content/drive/My Drive/dsr_project/data/arthur-sykes-rymer-auditorium-university-york/b-format/s1r2.wav\",\n",
        "           \"/content/drive/My Drive/dsr_project/data/trollers-gill/b-format/dales_site1_1way_bformat.wav\"]\n",
        "\n",
        "# function to convolve IRs with audio\n",
        "def my_convolve(ir, audio):\n",
        "  audio_ = torch.nn.functional.pad(audio, (ir.shape[1]-1, 0))\n",
        "  reverb_audio = torch.nn.functional.conv1d(audio_[None, ...], ir[None, ...])[0]\n",
        "  return reverb_audio\n",
        "\n",
        "# we number the IRs consecutively\n",
        "unique_labels = [i for i in range(len(irpaths))]\n",
        "\n",
        "# placeholder lists\n",
        "irs = []\n",
        "audios = []\n",
        "reverb_audios = []\n",
        "audio_chunks = []\n",
        "reverb_audio_chunks = []\n",
        "audio_chunks_2D = []\n",
        "all_labels = []\n",
        "labeled_reverb_audio_chunks = []\n",
        "\n",
        "# loop over all audiofiles without reverb\n",
        "for audiopath in audiopaths:\n",
        "  # load audio without reverb \n",
        "  audio, _ = my_get_sample(audiopath, resample=sample_rate)\n",
        "  #print(f\"audio.shape[1]: {audio.shape[1]}\")\n",
        "\n",
        "  # convert audio without reverb to numpy array\n",
        "  #numpy_audio = audio.numpy()\n",
        "\n",
        "  # collect alls audio without reverb in a list\n",
        "  audios.append(audio)\n",
        "\n",
        "  # split audio without reverb into chunks of length n\n",
        "  audio_chunks_1D = []\n",
        "  for j in range(0, audio.shape[1] - n, n):\n",
        "    # a single audio chunk without reverb of length n\n",
        "    audio_chunk = audio[:,j:j + n]\n",
        "    \n",
        "    # collect audio chunks without reverb in a list\n",
        "    audio_chunks_1D.append(audio_chunk.numpy())\n",
        "  \n",
        "  # collect audio chunks without reverb in a 2D list\n",
        "  audio_chunks_2D.append(audio_chunks_1D)\n",
        "\n",
        "\n",
        "# we convolve all audios with all IRs\n",
        "# counter for irpaths\n",
        "iridx = 0\n",
        "for irpath in irpaths:\n",
        "  # load IR functions\n",
        "  ir_raw, _ = my_get_ir_sample(irpath, resample=sample_rate)\n",
        "  \n",
        "  # shorten all IR functions to the same length\n",
        "  ir = ir_raw[:, 0:int(sample_rate*2.0)]\n",
        "  \n",
        "  # renormalize IRs\n",
        "  ir = ir / torch.norm(ir, p=2)\n",
        "  \n",
        "  # time reverse IRs\n",
        "  ir = torch.flip(ir, [1])\n",
        "  \n",
        "  # collect IRs in a list\n",
        "  irs.append(ir)\n",
        "  \n",
        "  # plot IRs\n",
        "  #plot_waveform(ir, sample_rate, title=\"Room Impulse Response\", ylim=None)\n",
        "  \n",
        "  # loop over all audios without reverb\n",
        "  # counter for audios\n",
        "  audioidx = 0\n",
        "  for audio in audios:\n",
        "\n",
        "    try:\n",
        "    \n",
        "      # convole audio without reverb with IRs to obtain audio with reverb\n",
        "      reverb_audio = my_convolve(ir, audio)\n",
        "      #print(f\"reverb_audio.shape[1]: {reverb_audio.shape[1]}\")\n",
        "      \n",
        "      # collect audios with reverb in a list\n",
        "      reverb_audios.append(reverb_audio)\n",
        "\n",
        "      # counter for chunks\n",
        "      chunkidx = 0\n",
        "      # split audio with reverb into chunks of length n\n",
        "      for j in range(0, reverb_audio.shape[1] - n, n):\n",
        "\n",
        "        # a single audio chunk with reverb of length n\n",
        "        reverb_audio_chunk = reverb_audio[:,j:j + n]\n",
        "        \n",
        "        # collect audio chunks with reverb in a list\n",
        "        reverb_audio_chunks.append(reverb_audio_chunk.numpy())\n",
        "\n",
        "        # collect audio chunks without reverb in a list\n",
        "        audio_chunks.append(audio_chunks_2D[audioidx][chunkidx])\n",
        "        \n",
        "        # collect corresponding labels in a list\n",
        "        all_labels.append(unique_labels[iridx])\n",
        "        \n",
        "        # collect tuples audio chunks with reverb and labels in a list\n",
        "        labeled_reverb_audio_chunk = (reverb_audio_chunk.numpy(), unique_labels[iridx])\n",
        "        labeled_reverb_audio_chunks.append(labeled_reverb_audio_chunk)\n",
        "        chunkidx = chunkidx +1\n",
        "\n",
        "    except: print(\"Failed to convert audio. Signal shorter than IR (?)\")\n",
        "    \n",
        "    # increase counter for audios\n",
        "    audioidx = audioidx +1\n",
        "  \n",
        "  # increase counter for irpaths\n",
        "  iridx = iridx + 1\n",
        "\n",
        "# convert list to numpy arrays\n",
        "features = np.array(reverb_audio_chunks)\n",
        "targets = np.array(audio_chunks)\n",
        "labels = np.array(all_labels)\n",
        "print(f\"features.shape: {features.shape}\")\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")\n",
        "\n",
        "# determine size of dataset\n",
        "dataset_size = labels.shape[0]\n",
        "\n",
        "# convert numpy arrays to tensorflow datasets\n",
        "# we can generate two different datasets:\n",
        "#   one with the labels as the targets\n",
        "#   and one with the sound without reverb as targets\n",
        "label_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "target_dataset = tf.data.Dataset.from_tensor_slices((features, targets))\n",
        "\n",
        "tf.data.experimental.save(label_dataset, path + \"tf_label_dataset\")\n",
        "tf.data.experimental.save(target_dataset, path + \"tf_target_dataset\")"
      ],
      "metadata": {
        "id": "KErcslplB-WW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fb103ae-9fa2-4481-bba7-1a361011fc17"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "features.shape: (750, 1, 64000)\n",
            "targets.shape: (750, 1, 64000)\n",
            "labels.shape: (750,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/\"\n",
        "label_dataset = tf.data.experimental.load(path + \"tf_label_dataset\")\n",
        "target_dataset = tf.data.experimental.load(path + \"tf_target_dataset\")\n",
        "dataset_size = sum(1 for _ in label_dataset)\n",
        "\n",
        "# shuffle the dataset before splitting in train and validate!\n",
        "label_dataset = label_dataset.shuffle(dataset_size)\n",
        "\n",
        "# split dataset in train and validate\n",
        "train_fraction = 0.8\n",
        "validate_dataset_size = int(dataset_size * (1.0-train_fraction)) # 20 percent of dataset_size\n",
        "train_dataset = label_dataset.skip(validate_dataset_size)\n",
        "validate_dataset = label_dataset.take(validate_dataset_size)"
      ],
      "metadata": {
        "id": "17nKgpGN7sRm"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}