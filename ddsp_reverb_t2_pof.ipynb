{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ddsp-reverb--t2-pof",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobatgithub/unreverb/blob/main/ddsp_reverb_t2_pof.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install --upgrade ddsp\n",
        "!pip install tensorflow_io\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "U6S-QyllLLOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nSh-eBHULsi"
      },
      "outputs": [],
      "source": [
        "sample_length = 16000\n",
        "# load data\n",
        "# path of the tensorflow dataset \n",
        "path = \"/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/\"\n",
        "\n",
        "# load the tensorflow dataset\n",
        "dataset = tf.data.experimental.load(path + \"tf_dataset\")\n",
        "\n",
        "# determine size of the dataset\n",
        "dataset_size = sum(1 for _ in dataset)\n",
        "print(f\"dataset_size: {dataset_size}\")\n",
        "\n",
        "def lambda_1(features, labels, targets):\n",
        "  return (features, labels)\n",
        "\n",
        "def lambda_2(features, labels, targets):\n",
        "  # features = features [:sample_length] \n",
        "  # targets = targets [:sample_length] \n",
        "  return (targets, features )  \n",
        "\n",
        "# obtain the datasets containing only labels or targets\n",
        "# label_dataset = dataset.map(lambda features, labels, targets: lambda_1(features, labels, targets))\n",
        "target_dataset = dataset.map(lambda features, labels, targets: lambda_2(features, labels, targets))\n",
        "\n",
        "# shuffle the dataset before splitting in train and validate!\n",
        "# target_dataset = target_dataset.shuffle(dataset_size)\n",
        "\n",
        "# split dataset in train and validate\n",
        "train_fraction = 0.8\n",
        "validate_dataset_size = int(dataset_size * (1.0-train_fraction)) # 20 percent of dataset_size\n",
        "train_dataset = target_dataset.skip(validate_dataset_size)\n",
        "validate_dataset = target_dataset.take(validate_dataset_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "for sample in train_dataset.shuffle(dataset_size).take(1):\n",
        "  audio = sample[0][0].numpy().astype(\"float32\") \n",
        "  label = sample[1].numpy()\n",
        "\n",
        "  # print(label)\n",
        "  # break\n",
        "  print(audio.shape)\n",
        "  plt.plot(audio)\n",
        "  plt.title(f\"Label: {label}\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  display(Audio(audio, rate=8000))\n",
        "\n",
        "  mel = librosa.feature.melspectrogram(\n",
        "      y=audio, n_mels=128, hop_length=64, sr=8000, fmax=2000\n",
        "  )\n",
        "\n",
        "  mel /= np.max(mel)\n",
        "  plt.imshow(mel[::-1, :], cmap=\"inferno\")"
      ],
      "metadata": {
        "id": "rCt80jUy1ohK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal.spectral import spectrogram\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def pad_second_dim(input, desired_size):\n",
        "    padding = tf.tile([[0]], tf.stack([tf.shape(input)[0], desired_size - tf.shape(input)[1]], 0))\n",
        "    return tf.concat([input, padding], 1)\n",
        "\n",
        "def preprocess(sample, target):\n",
        "  audio = sample[0]\n",
        "  label = target[0]\n",
        "  spectrogram = tfio.audio.spectrogram(audio, nfft=1024, window=1024, stride=64)\n",
        "  spectrogram = tfio.audio.melscale(spectrogram, rate=8000, mels=128, fmin=0, fmax=2000)\n",
        "  spectrogram /= tf.math.reduce_max(spectrogram)\n",
        "  spectrogram = tf.expand_dims(spectrogram, axis=-1)\n",
        "\n",
        "  spectrogram = tf.image.resize(spectrogram, (128, 128))\n",
        "  spectrogram = tf.transpose(spectrogram, perm=(1, 0, 2))\n",
        "  spectrogram = spectrogram[::-1, :, :]\n",
        "\n",
        "  return spectrogram, label\n",
        "\n",
        "def preprocess_both(sample, target):\n",
        "  audio = sample[0]\n",
        "  label = target[0]\n",
        "  outs = []\n",
        "  for audio in [audio, label]:\n",
        "    spectrogram = tfio.audio.spectrogram(audio, nfft=1024, window=1024, stride=64)\n",
        "    spectrogram = tfio.audio.melscale(spectrogram, rate=8000, mels=128, fmin=0, fmax=2000)\n",
        "    spectrogram /= tf.math.reduce_max(spectrogram)\n",
        "    spectrogram = tf.expand_dims(spectrogram, axis=-1)\n",
        "\n",
        "    spectrogram = tf.image.resize(spectrogram, (128, 128))\n",
        "    spectrogram = tf.transpose(spectrogram, perm=(1, 0, 2))\n",
        "    spectrogram = spectrogram[::-1, :, :]\n",
        "    outs.append(spectrogram)\n",
        "\n",
        "  return outs[0], outs[1]\n",
        "\n",
        "def preprocess3(sample, target):\n",
        "  audio = sample[0]\n",
        "  label = target[0]\n",
        "  audio = audio [:sample_length] \n",
        "  label = label [:sample_length] \n",
        "\n",
        "  return audio, label\n",
        "\n",
        "\n",
        "dataset_check = train_dataset.map(lambda sample, target: preprocess3(sample, target))\n",
        "\n",
        "# for x, y in dataset_check.shuffle(dataset_size).take(4):\n",
        "  # plt.imshow(x[:,:,0], cmap=\"inferno\")\n",
        "  # plt.plot(x)\n",
        "  # reconstruction = tfio.audio.inverse_spectrogram((x[:,:,0], nfft=1024, window=1024, stride=64, iterations=30)\n",
        "  # plt.plot(reconstruction)\n",
        "  # display(Audio(reconstruction, rate=8000))\n",
        "\n",
        "\n",
        "  # plt.title(f\"{x.shape}, label: {y.numpy()}\")\n",
        "  # plt.show()\n",
        "  # plt.close()"
      ],
      "metadata": {
        "id": "OCR3QncV35Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = train_dataset.map(lambda sample, target: preprocess3(target, sample))\n",
        "# dataset_train = train_dataset\n",
        "dataset_train = dataset_train.cache()\n",
        "dataset_train = dataset_train.shuffle(dataset_size)\n",
        "dataset_train = dataset_train.batch(32)\n",
        "\n",
        "dataset_validate = validate_dataset.map(lambda sample, target: preprocess3(target, sample))\n",
        "# dataset_validate = validate_dataset\n",
        "dataset_validate = dataset_validate.cache()\n",
        "dataset_validate = dataset_validate.batch(32)"
      ],
      "metadata": {
        "id": "gqODKt4XKszC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ddsp\n",
        "\n",
        "# Get synthesizer parameters from the input audio.\n",
        "# outputs = network(audio_input)\n",
        "\n",
        "# Initialize signal processors.\n",
        "# harmonic = ddsp.synths.Harmonic()\n",
        "# filtered_noise = ddsp.synths.FilteredNoise()\n",
        "# reverb = ddsp.effects.TrainableReverb()\n",
        "kwargs = {'return_outputs_dict': True}\n",
        "reverb = ddsp.effects.Reverb(trainable=True, reverb_length=16000, add_dry=True, )\n",
        "spectral_loss = ddsp.losses.SpectralLoss()\n",
        "\n",
        "# Generate audio.\n",
        "# audio_harmonic = harmonic(outputs['amplitudes'],\n",
        "#                           outputs['harmonic_distribution'],\n",
        "#                           outputs['f0_hz'])\n",
        "# audio_noise = filtered_noise(outputs['magnitudes'])\n",
        "# audio = audio_harmonic + audio_noise\n",
        "# audio = reverb(audio)\n",
        "\n",
        "# Multi-scale spectrogram reconstruction loss.\n",
        "# loss = spectral_loss(audio, audio_input)"
      ],
      "metadata": {
        "id": "AiYkD5DOSGJ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gin\n",
        "reverb = ddsp.effects.Reverb(trainable=True, reverb_length=16000, add_dry=True, )\n",
        "\n",
        "dag = [\n",
        "  (reverb,\n",
        "   ['add/signal']),\n",
        "]\n",
        "\n",
        "processor_group = ddsp.processors.ProcessorGroup(dag=dag)"
      ],
      "metadata": {
        "id": "TYpHSJEr98Fo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "  def get_signal(audio):\n",
        "    audio, ir = audio[0], audio[1]\n",
        "    \"\"\"Apply impulse response.\n",
        "    Args:\n",
        "      audio: Dry audio, 2-D Tensor of shape [batch, n_samples].\n",
        "      ir: 3-D Tensor of shape [batch, ir_size, 1] or 2D Tensor of shape\n",
        "        [batch, ir_size].\n",
        "    Returns:\n",
        "      tensor of shape [batch, n_samples]\n",
        "    \"\"\"\n",
        "    # audio, ir = tf_float32(audio), tf_float32(ir)\n",
        "    # ir = self._mask_dry_ir(ir)\n",
        "    wet = ddsp.core.fft_convolve(audio, ir, padding='same', delay_compensation=0)\n",
        "    return (wet + audio) #if self._add_dry else wet"
      ],
      "metadata": {
        "id": "g2zEeoVQD1hI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x, y in train_dataset.take(1):\n",
        "  print \n",
        "  x = reverb(x, x)\n"
      ],
      "metadata": {
        "id": "aKK2gQofmkaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "import tensorflow.keras as keras\n",
        "conv1d_filters = 16\n",
        "conv1d_strides = 3\n",
        "my_input_shape = (128, 128, 1)\n",
        "\n",
        "from tensorflow.python.keras.layers import Lambda;\n",
        "\n",
        "def rev(x):\n",
        "    x = ddsp.effects.Reverb(trainable=True, reverb_length=16000, add_dry=True,)(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "# encoder2 = models.Sequential()\n",
        "# model.add(layers.Input((32, 1, 64000)))\n",
        "# encoder2.add(layers.Reshape((16000,)))\n",
        "# encoder2.add(processor_group())\n",
        "# encoder2.add(reverb(training=True) )\n",
        "# encoder2.add(ddsp.effects.Reverb(trainable=True, reverb_length=16000, add_dry=True,))\n",
        "# model.add(layers.Flatten())\n",
        "# model.add(layers.Dense(3, activation=\"softmax\"))\n",
        "# encoder2.summary()\n",
        "\n",
        "encoder_input = keras.Input(shape=(1, 16000), name=\"audio\")\n",
        "audio, ir = encoder_input, encoder_input\n",
        "x = layers.Reshape((16000,))(audio)\n",
        "audio = x\n",
        "pack = (audio, ir)\n",
        "# x = reverb(input)\n",
        "# x = processor_group(x)\n",
        "# x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
        "# x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "# x = layers.MaxPooling2D(3)(x)\n",
        "# x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "# x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "# print(encoder_input.shape)\n",
        "# x = reverb(x, return_outputs_dict=True, training=True )  # , return_outputs_dict=True   [\"controls\"] [\"audio\"]\n",
        "# encoder_output = reverb(x, x, return_outputs_dict=True) [\"controls\"]\n",
        "# wet = ddsp.core.fft_convolve(audio, ir, padding='same', delay_compensation=0)\n",
        "\n",
        "x = Lambda(get_signal)(pack);\n",
        "encoder_output = layers.Reshape((1, 16000))(x);\n",
        "# encoder_output = ddsp.effects.Reverb(trainable=True, reverb_length=16000, add_dry=True,)(x)\n",
        "\n",
        "encoder2 = keras.Model(encoder_input, encoder_output, name=\"encoder2\" )\n",
        "# encoder.summary()\n",
        "\n",
        "\n",
        "\n",
        "encoder2.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss = spectral_loss,\n",
        "    # loss=tf.keras.losses.MeanSquaredError(),\n",
        "    # loss=\"binary_crossentropy\",\n",
        "    # loss=\"sparse_categorical_crossentropy\",\n",
        "    # metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = encoder2.fit(\n",
        "    dataset_train,\n",
        "    epochs=50,\n",
        "    # validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "xAQkmwEJTJFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = keras.Input(shape=(sample_length,), name=\"img\")\n",
        "audio = layers.Reshape((sample_length, ))(encoder_input)\n",
        "xim = ddsp.spectral_ops.stft(audio)\n",
        "xr = tf.math.real(xim)\n",
        "xi = tf.math.imag(xim)\n",
        "x = tf.stack((xr, xi), axis=-1)\n",
        "x = layers.Reshape((32, 1025, 2))(x)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "x = layers.Reshape((4, 4, ))(encoder_output)\n",
        "x = layers.Conv1DTranspose(16, 3, activation=\"relu\")(x)\n",
        "x = layers.UpSampling1D(10)(x)\n",
        "\n",
        "x = layers.Conv1DTranspose(32, 3, activation=\"relu\")(x)\n",
        "x = layers.UpSampling1D(10)(x)\n",
        "x = layers.Conv1DTranspose(16, 3, activation=\"relu\")(x)\n",
        "x = layers.UpSampling1D(10)(x)\n",
        "x = layers.Conv1DTranspose(1, 3, activation=\"relu\")(x)\n",
        "x = layers.Reshape((6222, ))(x)\n",
        "# x = layers.Flatten(x)\n",
        "x = layers.Dense( sample_length)(x)\n",
        "decoder_output = layers.Reshape((16000, ))(x + audio)\n",
        "\n",
        "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
        "autoencoder.summary()\n",
        "\n",
        "\n",
        "\n",
        "autoencoder.compile(\n",
        "    optimizer=\"adam\",\n",
        "    # loss=tf.keras.losses.MeanSquaredError(),\n",
        "    loss=ddsp.losses.SpectralLoss()\n",
        "    # loss=\"binary_crossentropy\",\n",
        "    # loss=\"sparse_categorical_crossentropy\",\n",
        "    # metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    dataset_train,\n",
        "    epochs=50,\n",
        "    validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "qvb9Lhq2LY-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "frame_length = 1024\n",
        "overlap = 0.9\n",
        "frame_step = int(frame_length * (1.0 - overlap))\n",
        "window_fn = tf.signal.hamming_window\n",
        "\n",
        "\n",
        "\n",
        "encoder_input = keras.Input(shape=(sample_length,), name=\"img\")\n",
        "audio = layers.Reshape((sample_length, ))(encoder_input)\n",
        "# xim = ddsp.spectral_ops.stft(audio)\n",
        "xim = tf.signal.stft(\n",
        "    audio, frame_length, frame_step, window_fn=window_fn)\n",
        "xr = tf.math.real(xim)\n",
        "xi = tf.math.imag(xim)\n",
        "xb = tf.stack((xr, xi), axis=-1)\n",
        "x = layers.Reshape((147, 513, 2))(xb)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "# encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "encoder.summary()\n",
        "x = layers.Reshape((43, 165, 16))(encoder_output)\n",
        "# x = layers.Reshape((4, 4, 1))(encoder_output)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
        "# x = layers.UpSampling2D(10)(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
        "\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
        "# x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
        "x = tf.dtypes.complex(\n",
        "    x[:,:,0], x[:,:,1], name=None\n",
        ")\n",
        "x = tf.signal.inverse_stft(\n",
        "    x, frame_length, frame_step, fft_length=None,\n",
        "    window_fn=window_fn, name=None\n",
        ")\n",
        "# x = layers.Reshape((6222, ))(x)\n",
        "# x = layers.Flatten(x)\n",
        "x = layers.Dense( sample_length)(x)\n",
        "decoder_output = layers.Reshape((16000, ))(x )  #  + audio\n",
        "\n",
        "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
        "autoencoder.summary()\n",
        "\n",
        "\n",
        "\n",
        "autoencoder.compile(\n",
        "    optimizer=\"adam\",\n",
        "    # loss=tf.keras.losses.MeanSquaredError(),\n",
        "    loss=ddsp.losses.SpectralLoss()\n",
        "    # loss=\"binary_crossentropy\",\n",
        "    # loss=\"sparse_categorical_crossentropy\",\n",
        "    # metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    dataset_train,\n",
        "    epochs=50,\n",
        "    validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "IKEBXPwRmz4b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.backend.clear_session()\n",
        "\n",
        "frame_length = 512\n",
        "overlap = 0.75\n",
        "frame_step = int(frame_length * (1.0 - overlap))\n",
        "window_fn = tf.signal.hamming_window\n",
        "\n",
        "\n",
        "\n",
        "encoder_input = keras.Input(shape=(sample_length,), name=\"img\")\n",
        "audio = layers.Reshape((sample_length, ))(encoder_input)\n",
        "# xim = ddsp.spectral_ops.stft(audio)\n",
        "xim = tf.signal.stft(\n",
        "    audio, frame_length, frame_step, window_fn=window_fn)\n",
        "xr = tf.math.real(xim)\n",
        "xi = tf.math.imag(xim)\n",
        "# xb = tf.stack((xr, xi), axis=-1) # axis=-1\n",
        "xb = tf.concat([tf.math.real(xim), tf.math.imag(xim)], axis=-1)\n",
        "x = tf.expand_dims(xb, 3)\n",
        "# x = layers.Reshape((28, 2050, 1))(xb)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(3)(x)\n",
        "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
        "# encoder_output = layers.GlobalMaxPooling2D()(x)\n",
        "encoder_output = x #xb\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "encoder.summary()\n",
        "x = layers.Reshape(encoder_output.shape[1:])(encoder_output)\n",
        "# x = layers.Reshape((4, 678, 16))(encoder_output)\n",
        "# x = layers.Reshape((4, 4, 1))(encoder_output)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
        "# x = layers.UpSampling2D(10)(x)\n",
        "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
        "\n",
        "x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
        "# x = layers.UpSampling2D(3)(x)\n",
        "x = layers.Conv2DTranspose(1, 3, activation=\"tanh\")(x)\n",
        "\n",
        "# x = layers.Reshape((58, 2050 ))(x)\n",
        "x = layers.Reshape((121, 514))(x)\n",
        "# x = tf.squeeze(x)\n",
        "\n",
        "# x = tf.dtypes.complex(\n",
        "    # encoder_output[:,:,0], encoder_output[:,:,1], name=None\n",
        "# )\n",
        "x = tf.dtypes.complex(\n",
        "    x[:, :, :257], x[:, :, 257:], name=None\n",
        ")\n",
        "\n",
        "x = tf.signal.inverse_stft(\n",
        "    x, frame_length, frame_step,\n",
        "    window_fn=tf.signal.inverse_stft_window_fn(\n",
        "       frame_step, forward_window_fn=window_fn))\n",
        "\n",
        "# x = layers.Reshape((6222, ))(x)\n",
        "# x = layers.Flatten(x)\n",
        "# x = tf.pad(x)\n",
        "# x = layers.Dense( sample_length)(x)\n",
        "\n",
        "paddings = [[0, 0], [0, sample_length-tf.shape(x)[1]]]\n",
        "x = tf.pad(x, paddings, 'CONSTANT', constant_values=0)\n",
        "\n",
        "# decoder_output = layers.Reshape((16000, ))(x )  #  + audio\n",
        "decoder_output = x \n",
        "\n",
        "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
        "autoencoder.summary()\n",
        "\n",
        "\n",
        "\n",
        "autoencoder.compile(\n",
        "    optimizer=\"adam\",\n",
        "    # loss=tf.keras.losses.MeanSquaredError(),\n",
        "    loss=ddsp.losses.SpectralLoss()\n",
        "    # loss=\"binary_crossentropy\",\n",
        "    # loss=\"sparse_categorical_crossentropy\",\n",
        "    # metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    dataset_train,\n",
        "    epochs=50,\n",
        "    validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "Zm517Vo84Yfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = autoencoder.fit(\n",
        "    dataset_train,\n",
        "    epochs=50,\n",
        "    validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "BqrbOEBMVtUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt \n",
        "from IPython.display import Audio, display\n",
        "\n",
        "sample_from_batch = 1\n",
        "\n",
        "for audio, target in dataset_train.take(1):\n",
        "  pred = autoencoder.predict(audio)[sample_from_batch]\n",
        "  plt.plot(pred, alpha=0.6)\n",
        "  # plt.show()\n",
        "  # plt.close()\n",
        "  plt.plot(target[sample_from_batch], alpha=0.6)\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "  display(Audio(pred, rate=8000))\n",
        "  display(Audio(target[sample_from_batch], rate=8000))\n",
        "  display(Audio(audio[sample_from_batch], rate=8000))\n",
        "  print(ddsp.losses.SpectralLoss()(pred, target))\n",
        "\n"
      ],
      "metadata": {
        "id": "6axB0i_SdCz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder_input = keras.Input(shape=(1, sample_length,), name=\"img\")\n",
        "x = layers.Reshape((sample_length, 1))(encoder_input)\n",
        "x = layers.Conv1D(16, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv1D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling1D(3)(x)\n",
        "x = layers.Conv1D(32, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv1D(16, 3, activation=\"relu\")(x)\n",
        "encoder_output = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
        "encoder.summary()\n",
        "\n",
        "x = layers.Reshape((4, 4, ))(encoder_output)\n",
        "x = layers.Conv1DTranspose(16, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv1DTranspose(32, 3, activation=\"relu\")(x)\n",
        "x = layers.UpSampling1D(3)(x)\n",
        "x = layers.Conv1DTranspose(16, 3, activation=\"relu\")(x)\n",
        "x = layers.Conv1DTranspose(1, 3, activation=\"relu\")(x)\n",
        "x = layers.Flatten(x)\n",
        "decoder_output = layers.Dense( sample_length)(x)\n",
        "\n",
        "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
        "autoencoder.summary()\n",
        "\n",
        "\n",
        "\n",
        "autoencoder.compile(\n",
        "    optimizer=\"adam\",\n",
        "    # loss=tf.keras.losses.MeanSquaredError(),\n",
        "    loss=ddsp.losses.SpectralLoss()\n",
        "    # loss=\"binary_crossentropy\",\n",
        "    # loss=\"sparse_categorical_crossentropy\",\n",
        "    # metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    dataset_train,\n",
        "    epochs=50,\n",
        "    # validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "LLzQqDLyemE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "conv1d_filters = 16\n",
        "conv1d_strides = 3\n",
        "my_input_shape = (128, 128, 1)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(4, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(3, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    # loss=tf.keras.losses.MeanSquaredError(),\n",
        "    # loss=\"binary_crossentropy\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_train,\n",
        "    epochs=50,\n",
        "    validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "lVwhbZsGO7YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_history(history):\n",
        "    plt.plot(history[\"loss\"], label=\"loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Our losses\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    plt.plot(history[\"accuracy\"], label=\"accuracy\")\n",
        "    plt.plot(history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Our accuracies\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def compare_histories():\n",
        "    for training_name, history in history_list.items():\n",
        "        plt.plot(history[\"val_accuracy\"], label=training_name)\n",
        "    plt.legend()\n",
        "    plt.title(\"Comparision of val_accuracy\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "render_history(history.history)"
      ],
      "metadata": {
        "id": "es2rcmRfPpPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}