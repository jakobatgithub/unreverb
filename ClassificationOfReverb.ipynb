{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationOfReverb.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyMkEsovOzoA2N/IMX7GflsB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobatgithub/unreverb/blob/main/ClassificationOfReverb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_io pyyaml h5py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "U6S-QyllLLOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "sample_rate = 2**14\n",
        "\n",
        "# function to delete all content in a folder\n",
        "def delete_folder_contents(folder):\n",
        "  for filename in os.listdir(folder):\n",
        "      file_path = os.path.join(folder, filename)\n",
        "      try:\n",
        "          if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "              os.unlink(file_path)\n",
        "          elif os.path.isdir(file_path):\n",
        "              shutil.rmtree(file_path)\n",
        "      except Exception as e:\n",
        "          print('Failed to delete %s. Reason: %s' % (file_path, e))  \n",
        "\n",
        "# load data\n",
        "path = '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/'\n",
        "save_path_M = path + 'datasets_randIR_M/'\n",
        "save_path_F = path + 'datasets_randIR_F/'\n",
        "\n",
        "dataset_filenames_M = glob.glob(save_path_M + 'tf_IR_*')\n",
        "dataset_filenames_F = glob.glob(save_path_F + 'tf_IR_*')\n",
        "dataset_filenames = dataset_filenames_M + dataset_filenames_F\n",
        "#labels = [int(filename.split('_')[-1]) for filename in dataset_filenames]\n",
        "#print(f\"labels: {labels}\")\n",
        "\n",
        "datasets = []\n",
        "for filename in dataset_filenames:\n",
        "  # load the tensorflow dataset\n",
        "  datasets.append(tf.data.experimental.load(filename))\n",
        "\n",
        "dataset = datasets[0]\n",
        "#for i in range(1, len(datasets)):\n",
        "for i in range(1, 10):\n",
        "  dataset = dataset.concatenate(datasets[i])\n",
        "\n",
        "# determine size of the dataset\n",
        "dataset_size = sum(1 for _ in dataset)\n",
        "print(f\"dataset_size: {dataset_size}\")"
      ],
      "metadata": {
        "id": "S4wUkdRfliuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nSh-eBHULsi"
      },
      "outputs": [],
      "source": [
        "def lambda_1(features, labels, targets):\n",
        "  return (features, labels)\n",
        "\n",
        "def lambda_2(features, labels, targets):\n",
        "  return (features, targets)\n",
        "\n",
        "def lambda_3(features, labels, targets):\n",
        "  return labels\n",
        "\n",
        "# obtain the datasets containing only labels or targets\n",
        "label_dataset = dataset.map(lambda features, labels, targets: lambda_1(features, labels, targets))\n",
        "target_dataset = dataset.map(lambda features, labels, targets: lambda_2(features, labels, targets))\n",
        "\n",
        "# obtain all labels\n",
        "all_labels = dataset.map(lambda features, labels, targets: lambda_3(features, labels, targets))\n",
        "\n",
        "# transform all_labels dataset to numpy array\n",
        "all_labels_np = []\n",
        "for label in all_labels:\n",
        "  all_labels_np.append(tfds.as_numpy(label))\n",
        "\n",
        "all_labels_np = np.array(all_labels_np)\n",
        "\n",
        "# determine unique labels\n",
        "unique_labels, counts = np.unique(all_labels_np, return_counts=True)\n",
        "print(f\"unique_labels: {unique_labels}\")\n",
        "#print(counts)\n",
        "\n",
        "# determine number of unique labels\n",
        "nr_of_unique_labels = unique_labels.shape[0]\n",
        "print(f\"nr_of_unique_labels: {nr_of_unique_labels}\")\n",
        "\n",
        "# shuffle the dataset before splitting in train and validate!\n",
        "label_dataset = label_dataset.shuffle(dataset_size)\n",
        "\n",
        "# split dataset in train and validate\n",
        "train_fraction = 0.8\n",
        "validate_dataset_size = int(dataset_size * (1.0-train_fraction)) # 20 percent of dataset_size\n",
        "train_dataset = label_dataset.skip(validate_dataset_size)\n",
        "validate_dataset = label_dataset.take(validate_dataset_size)\n",
        "\n",
        "total_dataset = dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "for audio, label  in train_dataset.shuffle(dataset_size).take(1):\n",
        "  audio = audio.numpy().astype(\"float32\")\n",
        "  label = label.numpy()\n",
        "\n",
        "  plt.plot(audio)\n",
        "  plt.title(f\"Label: {label}\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  display(Audio(audio, rate=sample_rate))\n",
        "\n",
        "  mel = librosa.feature.melspectrogram(\n",
        "      y=audio, n_mels=128, hop_length=64, sr=sample_rate, fmax=2000\n",
        "  )\n",
        "\n",
        "  mel /= np.max(mel)\n",
        "  plt.imshow(mel[::-1, :], cmap=\"inferno\")"
      ],
      "metadata": {
        "id": "rCt80jUy1ohK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal.spectral import spectrogram\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def preprocess(sample, target):\n",
        "  audio = tf.cast(sample, tf.float32)\n",
        "  label = target\n",
        "  #print(audio)\n",
        "  #print(label)\n",
        "  spectrogram = tfio.audio.spectrogram(audio, nfft=1024, window=1024, stride=64)\n",
        "  spectrogram = tfio.audio.melscale(spectrogram, rate=sample_rate, mels=128, fmin=0, fmax=2000)\n",
        "  spectrogram /= tf.math.reduce_max(spectrogram)\n",
        "  spectrogram = tf.expand_dims(spectrogram, axis=-1)\n",
        "\n",
        "  spectrogram = tf.image.resize(spectrogram, (128, 128))\n",
        "  spectrogram = tf.transpose(spectrogram, perm=(1, 0, 2))\n",
        "  spectrogram = spectrogram[::-1, :, :]\n",
        "\n",
        "  return spectrogram, label\n",
        "\n",
        "\n",
        "dataset_check = train_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "\n",
        "for x, y in dataset_check.shuffle(dataset_size).take(4):\n",
        "  plt.imshow(x[:,:,0], cmap=\"inferno\")\n",
        "  plt.title(f\"{x.shape}, label: {y.numpy()}\")\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "OCR3QncV35Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = train_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "dataset_train = dataset_train.cache()\n",
        "dataset_train = dataset_train.shuffle(dataset_size)\n",
        "dataset_train = dataset_train.batch(32)\n",
        "\n",
        "dataset_validate = validate_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "dataset_validate = dataset_validate.cache()\n",
        "dataset_validate = dataset_validate.batch(32)"
      ],
      "metadata": {
        "id": "gqODKt4XKszC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "my_input_shape = (128, 128, 1)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(4, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(nr_of_unique_labels, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    #loss=tf.keras.losses.MeanSquaredError(),\n",
        "    #loss=\"binary_crossentropy\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    #loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_train,\n",
        "    epochs=400,\n",
        "    validation_data=dataset_validate\n",
        ")\n",
        "\n",
        "model.save(save_path_F + 'model_randIR.h5')"
      ],
      "metadata": {
        "id": "lVwhbZsGO7YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(save_path_F + 'model_randIR.h5')"
      ],
      "metadata": {
        "id": "b-MwhTx6sHhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(\n",
        "    dataset_train,\n",
        "    epochs=100,\n",
        "    validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "OsuNrDoLiDPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(save_path_F + 'model_randIR.h5')"
      ],
      "metadata": {
        "id": "OIy4WomhnjtI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_history(history):\n",
        "    plt.plot(history[\"loss\"], label=\"loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Our losses\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    plt.plot(history[\"accuracy\"], label=\"accuracy\")\n",
        "    plt.plot(history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Our accuracies\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def compare_histories():\n",
        "    for training_name, history in history_list.items():\n",
        "        plt.plot(history[\"val_accuracy\"], label=training_name)\n",
        "    plt.legend()\n",
        "    plt.title(\"Comparison of val_accuracy\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "render_history(history.history)"
      ],
      "metadata": {
        "id": "es2rcmRfPpPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}