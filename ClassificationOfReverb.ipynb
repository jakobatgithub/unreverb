{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationOfReverb.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOkq2AKxYMaAVfaKRrvDPLL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobatgithub/unreverb/blob/main/ClassificationOfReverb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_io\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "U6S-QyllLLOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nSh-eBHULsi"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "# paths of the tensorflow datasets\n",
        "path_1 = '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/'\n",
        "path_2 = '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/'\n",
        "path_3 = '/content/drive/My Drive/dsr_project/data/HarvardWordList/'\n",
        "path_4 = '/content/drive/My Drive/dsr_project/data/Anechoic/'\n",
        "\n",
        "# load the tensorflow datasets\n",
        "dataset_1 = tf.data.experimental.load(path_1 + \"M_tf_randomIRFs_dataset\")\n",
        "dataset_2 = tf.data.experimental.load(path_2 + \"F_tf_randomIRFs_dataset\")\n",
        "dataset_3 = tf.data.experimental.load(path_3 + \"tf_randomIRFs_dataset\")\n",
        "dataset_4 = tf.data.experimental.load(path_4 + \"tf_randomIRFs_dataset\")\n",
        "dataset = dataset_1.concatenate(dataset_2)\n",
        "dataset = dataset.concatenate(dataset_3)\n",
        "dataset = dataset.concatenate(dataset_4)\n",
        "\n",
        "# determine size of the dataset\n",
        "dataset_size = sum(1 for _ in dataset)\n",
        "print(f\"dataset_size: {dataset_size}\")\n",
        "\n",
        "def lambda_1(features, labels, targets):\n",
        "  return (features, labels)\n",
        "\n",
        "def lambda_2(features, labels, targets):\n",
        "  return (features, targets)\n",
        "\n",
        "def lambda_3(features, labels, targets):\n",
        "  return labels\n",
        "\n",
        "# obtain the datasets containing only labels or targets\n",
        "label_dataset = dataset.map(lambda features, labels, targets: lambda_1(features, labels, targets))\n",
        "target_dataset = dataset.map(lambda features, labels, targets: lambda_2(features, labels, targets))\n",
        "\n",
        "# obtain all labels\n",
        "all_labels = dataset.map(lambda features, labels, targets: lambda_3(features, labels, targets))\n",
        "\n",
        "# transform all_labels dataset to numpy array\n",
        "all_labels_np = []\n",
        "for label in all_labels:\n",
        "  all_labels_np.append(tfds.as_numpy(label))\n",
        "\n",
        "all_labels_np = np.array(all_labels_np)\n",
        "\n",
        "# determine unique labels\n",
        "unique_labels, counts = np.unique(all_labels_np, return_counts=True)\n",
        "print(f\"unique_labels: {unique_labels}\")\n",
        "#print(counts)\n",
        "\n",
        "# determine number of unique labels\n",
        "nr_of_unique_labels = unique_labels.shape[0]\n",
        "print(f\"nr_of_unique_labels: {nr_of_unique_labels}\")\n",
        "\n",
        "# shuffle the dataset before splitting in train and validate!\n",
        "label_dataset = label_dataset.shuffle(dataset_size)\n",
        "\n",
        "# split dataset in train and validate\n",
        "train_fraction = 0.8\n",
        "validate_dataset_size = int(dataset_size * (1.0-train_fraction)) # 20 percent of dataset_size\n",
        "train_dataset = label_dataset.skip(validate_dataset_size)\n",
        "validate_dataset = label_dataset.take(validate_dataset_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "for sample in train_dataset.shuffle(dataset_size).take(1):\n",
        "  audio = sample[0][0].numpy().astype(\"float32\")\n",
        "  label = sample[1].numpy()\n",
        "\n",
        "  # print(label)\n",
        "  # break\n",
        "\n",
        "  plt.plot(audio)\n",
        "  plt.title(f\"Label: {label}\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  display(Audio(audio, rate=8000))\n",
        "\n",
        "  mel = librosa.feature.melspectrogram(\n",
        "      y=audio, n_mels=128, hop_length=64, sr=8000, fmax=2000\n",
        "  )\n",
        "\n",
        "  mel /= np.max(mel)\n",
        "  plt.imshow(mel[::-1, :], cmap=\"inferno\")"
      ],
      "metadata": {
        "id": "rCt80jUy1ohK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal.spectral import spectrogram\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def pad_second_dim(input, desired_size):\n",
        "    padding = tf.tile([[0]], tf.stack([tf.shape(input)[0], desired_size - tf.shape(input)[1]], 0))\n",
        "    return tf.concat([input, padding], 1)\n",
        "\n",
        "def preprocess(sample, target):\n",
        "  audio = sample[0]\n",
        "  label = target\n",
        "  spectrogram = tfio.audio.spectrogram(audio, nfft=1024, window=1024, stride=64)\n",
        "  spectrogram = tfio.audio.melscale(spectrogram, rate=8000, mels=128, fmin=0, fmax=2000)\n",
        "  spectrogram /= tf.math.reduce_max(spectrogram)\n",
        "  spectrogram = tf.expand_dims(spectrogram, axis=-1)\n",
        "\n",
        "  spectrogram = tf.image.resize(spectrogram, (128, 128))\n",
        "  spectrogram = tf.transpose(spectrogram, perm=(1, 0, 2))\n",
        "  spectrogram = spectrogram[::-1, :, :]\n",
        "\n",
        "  return spectrogram, label\n",
        "\n",
        "\n",
        "dataset_check = train_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "\n",
        "for x, y in dataset_check.shuffle(dataset_size).take(4):\n",
        "  plt.imshow(x[:,:,0], cmap=\"inferno\")\n",
        "  plt.title(f\"{x.shape}, label: {y.numpy()}\")\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "OCR3QncV35Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = train_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "dataset_train = dataset_train.cache()\n",
        "dataset_train = dataset_train.shuffle(dataset_size)\n",
        "dataset_train = dataset_train.batch(32)\n",
        "\n",
        "dataset_validate = validate_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "dataset_validate = dataset_validate.cache()\n",
        "dataset_validate = dataset_validate.batch(32)"
      ],
      "metadata": {
        "id": "gqODKt4XKszC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "conv1d_filters = 16\n",
        "conv1d_strides = 3\n",
        "my_input_shape = (128, 128, 1)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(4, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(nr_of_unique_labels, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    # loss=tf.keras.losses.MeanSquaredError(),\n",
        "    # loss=\"binary_crossentropy\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_train,\n",
        "    epochs=300,\n",
        "    validation_data=dataset_validate\n",
        ")"
      ],
      "metadata": {
        "id": "lVwhbZsGO7YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_history(history):\n",
        "    plt.plot(history[\"loss\"], label=\"loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Our losses\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    plt.plot(history[\"accuracy\"], label=\"accuracy\")\n",
        "    plt.plot(history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Our accuracies\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def compare_histories():\n",
        "    for training_name, history in history_list.items():\n",
        "        plt.plot(history[\"val_accuracy\"], label=training_name)\n",
        "    plt.legend()\n",
        "    plt.title(\"Comparision of val_accuracy\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "render_history(history.history)"
      ],
      "metadata": {
        "id": "es2rcmRfPpPd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}