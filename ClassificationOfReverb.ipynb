{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ClassificationOfReverb.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPcAQ3/01AxaOEgaVletFAz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobatgithub/unreverb/blob/main/ClassificationOfReverb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow_io pyyaml h5py\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "U6S-QyllLLOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import os\n",
        "\n",
        "sample_rate = 2**14\n",
        "\n",
        "# function to delete all content in a folder\n",
        "def delete_folder_contents(folder):\n",
        "  for filename in os.listdir(folder):\n",
        "      file_path = os.path.join(folder, filename)\n",
        "      try:\n",
        "          if os.path.isfile(file_path) or os.path.islink(file_path):\n",
        "              os.unlink(file_path)\n",
        "          elif os.path.isdir(file_path):\n",
        "              shutil.rmtree(file_path)\n",
        "      except Exception as e:\n",
        "          print('Failed to delete %s. Reason: %s' % (file_path, e))  \n",
        "\n",
        "# load data\n",
        "path = '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/'\n",
        "save_path = path + 'datasets_M/'\n",
        "checkpoint_path = 'training_1/cp.ckpt'\n",
        "checkpoint_dir = os.path.dirname(save_path + checkpoint_path)\n",
        "\n",
        "dataset_filenames = glob.glob(save_path + 'tf_IR_*')\n",
        "#labels = [int(filename.split('_')[-1]) for filename in dataset_filenames]\n",
        "#print(f\"labels: {labels}\")\n",
        "\n",
        "datasets = []\n",
        "for filename in dataset_filenames:\n",
        "  # load the tensorflow dataset\n",
        "  datasets.append(tf.data.experimental.load(filename))\n",
        "\n",
        "dataset = datasets[0]\n",
        "#for i in range(1, len(datasets)):\n",
        "for i in range(1, 10):\n",
        "  dataset = dataset.concatenate(datasets[i])\n",
        "\n",
        "# determine size of the dataset\n",
        "dataset_size = sum(1 for _ in dataset)\n",
        "print(f\"dataset_size: {dataset_size}\")\n",
        "\n",
        "#for idx in range(len(target_datasets)):\n",
        "#  target_dataset = target_datasets[idx]\n",
        "#  for features, targets in target_dataset:\n",
        "#    features = tfds.as_numpy(features)\n",
        "#    targets = tfds.as_numpy(targets)\n",
        "#    label_array = np.full(features.shape, labels[idx])\n",
        "#    dataset = tf.data.Dataset.from_tensor_slices((features, label_array, targets))\n",
        "#    delete_folder_contents(dataset_filenames[idx])\n",
        "#    tf.data.experimental.save(dataset, save_path + 'tf_IR_' + str(labels[idx]))"
      ],
      "metadata": {
        "id": "S4wUkdRfliuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nSh-eBHULsi"
      },
      "outputs": [],
      "source": [
        "def lambda_1(features, labels, targets):\n",
        "  return (features, labels)\n",
        "\n",
        "def lambda_2(features, labels, targets):\n",
        "  return (features, targets)\n",
        "\n",
        "def lambda_3(features, labels, targets):\n",
        "  return labels\n",
        "\n",
        "# obtain the datasets containing only labels or targets\n",
        "label_dataset = dataset.map(lambda features, labels, targets: lambda_1(features, labels, targets))\n",
        "target_dataset = dataset.map(lambda features, labels, targets: lambda_2(features, labels, targets))\n",
        "\n",
        "# obtain all labels\n",
        "all_labels = dataset.map(lambda features, labels, targets: lambda_3(features, labels, targets))\n",
        "\n",
        "# transform all_labels dataset to numpy array\n",
        "all_labels_np = []\n",
        "for label in all_labels:\n",
        "  all_labels_np.append(tfds.as_numpy(label))\n",
        "\n",
        "all_labels_np = np.array(all_labels_np)\n",
        "\n",
        "# determine unique labels\n",
        "unique_labels, counts = np.unique(all_labels_np, return_counts=True)\n",
        "print(f\"unique_labels: {unique_labels}\")\n",
        "#print(counts)\n",
        "\n",
        "# determine number of unique labels\n",
        "nr_of_unique_labels = unique_labels.shape[0]\n",
        "print(f\"nr_of_unique_labels: {nr_of_unique_labels}\")\n",
        "\n",
        "# shuffle the dataset before splitting in train and validate!\n",
        "label_dataset = label_dataset.shuffle(dataset_size)\n",
        "\n",
        "# split dataset in train and validate\n",
        "train_fraction = 0.8\n",
        "validate_dataset_size = int(dataset_size * (1.0-train_fraction)) # 20 percent of dataset_size\n",
        "train_dataset = label_dataset.skip(validate_dataset_size)\n",
        "validate_dataset = label_dataset.take(validate_dataset_size)\n",
        "train_total_dataset = dataset.skip(validate_dataset_size)\n",
        "validate_total_dataset = dataset.take(validate_dataset_size)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_total_dataset = dataset.skip(validate_dataset_size)\n",
        "validate_total_dataset = dataset.take(validate_dataset_size)"
      ],
      "metadata": {
        "id": "AXGjz_3x3a3k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio, display\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "for audio, label  in train_dataset.shuffle(dataset_size).take(1):\n",
        "  audio = audio.numpy().astype(\"float32\")\n",
        "  label = label.numpy()\n",
        "\n",
        "  plt.plot(audio)\n",
        "  plt.title(f\"Label: {label}\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  display(Audio(audio, rate=sample_rate))\n",
        "\n",
        "  mel = librosa.feature.melspectrogram(\n",
        "      y=audio, n_mels=128, hop_length=64, sr=sample_rate, fmax=2000\n",
        "  )\n",
        "\n",
        "  mel /= np.max(mel)\n",
        "  plt.imshow(mel[::-1, :], cmap=\"inferno\")"
      ],
      "metadata": {
        "id": "rCt80jUy1ohK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.signal.spectral import spectrogram\n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def preprocess(sample, target):\n",
        "  audio = tf.cast(sample, tf.float32)\n",
        "  label = target\n",
        "  #print(audio)\n",
        "  #print(label)\n",
        "  spectrogram = tfio.audio.spectrogram(audio, nfft=1024, window=1024, stride=64)\n",
        "  spectrogram = tfio.audio.melscale(spectrogram, rate=sample_rate, mels=128, fmin=0, fmax=2000)\n",
        "  spectrogram /= tf.math.reduce_max(spectrogram)\n",
        "  spectrogram = tf.expand_dims(spectrogram, axis=-1)\n",
        "\n",
        "  spectrogram = tf.image.resize(spectrogram, (128, 128))\n",
        "  spectrogram = tf.transpose(spectrogram, perm=(1, 0, 2))\n",
        "  spectrogram = spectrogram[::-1, :, :]\n",
        "\n",
        "  return spectrogram, label\n",
        "\n",
        "\n",
        "dataset_check = train_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "\n",
        "for x, y in dataset_check.shuffle(dataset_size).take(4):\n",
        "  plt.imshow(x[:,:,0], cmap=\"inferno\")\n",
        "  plt.title(f\"{x.shape}, label: {y.numpy()}\")\n",
        "  plt.show()\n",
        "  plt.close()"
      ],
      "metadata": {
        "id": "OCR3QncV35Ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_train = train_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "dataset_train = dataset_train.cache()\n",
        "dataset_train = dataset_train.shuffle(dataset_size)\n",
        "dataset_train = dataset_train.batch(32)\n",
        "\n",
        "dataset_validate = validate_dataset.map(lambda sample, target: preprocess(sample, target))\n",
        "dataset_validate = dataset_validate.cache()\n",
        "dataset_validate = dataset_validate.batch(32)"
      ],
      "metadata": {
        "id": "gqODKt4XKszC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import models, layers\n",
        "\n",
        "my_input_shape = (128, 128, 1)\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(4, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(8, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(16, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=my_input_shape))\n",
        "model.add(layers.MaxPool2D((2, 2)))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(nr_of_unique_labels, activation=\"softmax\"))\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    #loss=tf.keras.losses.MeanSquaredError(),\n",
        "    #loss=\"binary_crossentropy\",\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    #loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_train,\n",
        "    epochs=100,\n",
        "    validation_data=dataset_validate\n",
        ")\n",
        "model.save(save_path + 'my_model.h5')"
      ],
      "metadata": {
        "id": "lVwhbZsGO7YA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model = load_model(save_path + 'my_model.h5')"
      ],
      "metadata": {
        "id": "ksh7ja4vlLjw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(save_path + 'my_model.h5')"
      ],
      "metadata": {
        "id": "PZGAdReTkny7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def render_history(history):\n",
        "    plt.plot(history[\"loss\"], label=\"loss\")\n",
        "    plt.plot(history[\"val_loss\"], label=\"val_loss\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Our losses\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    plt.plot(history[\"accuracy\"], label=\"accuracy\")\n",
        "    plt.plot(history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Our accuracies\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def compare_histories():\n",
        "    for training_name, history in history_list.items():\n",
        "        plt.plot(history[\"val_accuracy\"], label=training_name)\n",
        "    plt.legend()\n",
        "    plt.title(\"Comparison of val_accuracy\")\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "render_history(history.history)"
      ],
      "metadata": {
        "id": "es2rcmRfPpPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_validate = []\n",
        "y_validate = []\n",
        "for sample, label in dataset_validate:\n",
        "  X_validate.append(sample.numpy())\n",
        "  y_validate.append(label.numpy())\n",
        "\n",
        "probabilities = []\n",
        "for X in X_validate:\n",
        "  probabilities.extend(model.predict(X))\n",
        "\n",
        "probabilities = np.asarray(probabilities)  "
      ],
      "metadata": {
        "id": "DORbXifYLnd_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_probabilities = []\n",
        "for probs in probabilities:\n",
        "  max_probabilities.append(np.max(probs))\n",
        "\n",
        "min_max_probabilities = np.min(max_probabilities)\n",
        "print(min_max_probabilities)\n",
        "binned_probs, bins, patches = plt.hist(max_probabilities, bins=10)\n",
        "plt.show()\n",
        "print(binned_probs)\n",
        "print(bins)"
      ],
      "metadata": {
        "id": "UfYe4QM2QC-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_validate = []\n",
        "targets_validate = []\n",
        "for sample, lable, target in validate_total_dataset:\n",
        "  samples_validate.append(sample.numpy())\n",
        "  targets_validate.append(target.numpy())\n",
        "\n",
        "samples_validate = np.asarray(samples_validate)\n",
        "targets_validate = np.asarray(targets_validate)"
      ],
      "metadata": {
        "id": "3hx9DIRq0JuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(samples_validate.shape)\n",
        "print(targets_validate.shape)"
      ],
      "metadata": {
        "id": "suzkSSJm7FxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display(Audio(samples_validate[0], rate=sample_rate))\n",
        "display(Audio(targets_validate[0], rate=sample_rate))"
      ],
      "metadata": {
        "id": "5NTUR5qR25Yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "p1 = probabilities[0][1]\n",
        "print(sum(p1))\n",
        "tol = 1e-4 \n",
        "p1[p1 < tol] = 0.0\n",
        "p1 = p1/sum(p1)\n",
        "print(sum(p1))"
      ],
      "metadata": {
        "id": "xeWmsgA4Op-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 0\n",
        "for audio, label in train_dataset.take(10):\n",
        "  audio = audio.numpy().astype(\"float32\")\n",
        "  me = np.sqrt(np.dot(audio, audio)/(audio.shape[0])**2)\n",
        "\n",
        "  plt.plot(audio)\n",
        "  plt.title(f\"Label: {label}, idx: {idx}, mean error: {me:.1E}\")\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "  idx = idx + 1"
      ],
      "metadata": {
        "id": "Vx5k5jGxR8yl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import glob, os\n",
        "import numpy as np\n",
        "import librosa\n",
        "\n",
        "irpaths = glob.glob(save_path + 'IR_*.wav')\n",
        "\n",
        "def wiener_deconvolution(signal, kernel, lambd=1e-3):\n",
        "  kernel = np.hstack((kernel, np.zeros(len(signal) - len(kernel)))) # zero pad the kernel to same length\n",
        "  H = fft(kernel)\n",
        "  Y = fft(signal)\n",
        "  #S = np.abs(fft(signal))**2\n",
        "  #GY = Y*np.conj(H)*S/(H*np.conj(H)*S + lambd**2)\n",
        "  GY = Y*np.conj(H)/(H*np.conj(H) + lambd**2)\n",
        "  deconvolved = np.real(ifft(GY))\n",
        "  return deconvolved\n",
        "\n",
        "def my_conv(signal, kernel):\n",
        "  kernel_padded = np.hstack((kernel, np.zeros(len(signal) - len(kernel))))\n",
        "  convolved_signal = np.real(ifft(fft(kernel_padded)*fft(signal)))\n",
        "  return convolved_signal\n",
        "\n",
        "irs = []\n",
        "for irpath in irpaths:\n",
        "  ir, IR_sample_rate = librosa.load(irpath, sr=sample_rate)\n",
        "  irs.append(ir)\n",
        "\n",
        "all_lengths = []\n",
        "for ir in irs:\n",
        "  all_lengths.append(len(ir))\n",
        "\n",
        "max_length = max(all_lengths)    "
      ],
      "metadata": {
        "id": "7uy95UdTUtHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probs = np.random.uniform(low=0, high=1, size=len(irs))\n",
        "probs = probs/sum(probs)\n",
        "\n",
        "my_sum = np.zeros(max_length)\n",
        "for idx in range(len(irs)):\n",
        "  ir = np.hstack((irs[idx], np.zeros(max_length - len(irs[idx]))))\n",
        "  my_sum = my_sum + probabilities[0][idx]*ir\n",
        "  \n",
        "my_sum"
      ],
      "metadata": {
        "id": "WHxGlz9g8Opv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}