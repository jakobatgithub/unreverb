{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PreprocessAndSaveData_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jakobatgithub/unreverb/blob/main/PreprocessAndSaveData_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "FlbhZHXidBAH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#path = '/content/drive/My Drive/dsr_project/data/Anechoic/'\n",
        "#audiopaths = !ls '/content/drive/My Drive/dsr_project/data/Anechoic/'*'/mono/'*'.wav'\n",
        "#audiopaths = [x.replace(\"\\'\", \"\") for x in audiopaths]"
      ],
      "metadata": {
        "id": "VlSL6nfqaIo6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/'\n",
        "audiopaths = !ls '/content/drive/My Drive/dsr_project/data/but-czas_v1.0/wavs/F'*'/'*'.wav'\n",
        "audiopaths = [x.replace(\"\\'\", \"\") for x in audiopaths]\n",
        "audiopaths"
      ],
      "metadata": {
        "id": "-JCZXqmQzRIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#irpaths = !ls '/content/drive/My Drive/dsr_project/data/random_IRs/IRF'*'.wav'\n",
        "#irpaths = [x.replace(\"\\'\", \"\") for x in irpaths]\n",
        "#irpaths"
      ],
      "metadata": {
        "id": "mY6eO5o-TuAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "irpaths = !ls '/content/drive/My Drive/dsr_project/data/samplicity-ir/'*'Rooms'*[0-1][1-9]*'/'*'M-to-S.wav'\n",
        "irpaths = [x.replace(\"\\'\", \"\") for x in irpaths]\n",
        "irpaths"
      ],
      "metadata": {
        "id": "BiH39f4EVkY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# this is done here:\n",
        "#   we load the audio data without reverb - the input\n",
        "#   we load the impulse response (IR) functions\n",
        "#   we generate the audio data with reverb by convolving all impulse response functions with all audio data without reverb\n",
        "#   we transform all data to tensorflow datasets\n",
        "#   one can solve two different tasks:\n",
        "#     classification: which impulse response function is used\n",
        "#     regression: remove the reverb from the input\n",
        "#   correspondingly, we can generate two different datasets here:\n",
        "#     one with the labels (consequtive numbers of the IRs) as the targets for classification\n",
        "#     one with the audios without reverb as the targets (commented out)\n",
        "\n",
        "import numpy as np\n",
        "import librosa\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import Audio \n",
        "from IPython.core.display import display\n",
        "from numpy.fft import fft, ifft\n",
        "import tensorflow as tf\n",
        "\n",
        "# sampling rate for resampling\n",
        "sample_rate = 2**14\n",
        "\n",
        "# all audio input should have the same length\n",
        "# we split all audio data into chunks of length n\n",
        "# n = 2*sample_rate corresponds to 2 seconds\n",
        "# correspondingly, all audio data without reverb must be at least 2 seconds long\n",
        "n = sample_rate\n",
        "\n",
        "# paths for audio without reverb\n",
        "audiopaths = ['/content/drive/My Drive/dsr_project/data/HarvardWordList/SIHarvardWordListsFemale.wav',\n",
        "              '/content/drive/My Drive/dsr_project/data/HarvardWordList/SIHarvardWordListsMale.wav']\n",
        "\n",
        "# paths to IR functions\n",
        "#irpaths = [\n",
        "#           \"/content/drive/My Drive/dsr_project/data/r1-nuclear-reactor-hall/b-format/r1_bformat.wav\", \n",
        "#           \"/content/drive/My Drive/dsr_project/data/arthur-sykes-rymer-auditorium-university-york/b-format/s1r2.wav\",\n",
        "#           \"/content/drive/My Drive/dsr_project/data/trollers-gill/b-format/dales_site1_1way_bformat.wav\"]\n",
        "\n",
        "# function to convolve IRs with audio\n",
        "def my_conv(signal, kernel):\n",
        "  kernel_padded = np.hstack((kernel, np.zeros(len(signal) - len(kernel))))\n",
        "  convolved_signal = np.real(ifft(fft(kernel_padded)*fft(signal)))\n",
        "  return convolved_signal\n",
        "\n",
        "# we number the IRs consecutively\n",
        "unique_labels = [i for i in range(len(irpaths))]\n",
        "\n",
        "# placeholder lists\n",
        "irs = []\n",
        "audios = []\n",
        "reverb_audios = []\n",
        "audio_chunks = []\n",
        "reverb_audio_chunks = []\n",
        "audio_chunks_2D = []\n",
        "all_labels = []\n",
        "labeled_reverb_audio_chunks = []\n",
        "\n",
        "# loop over all audiofiles without reverb\n",
        "for audiopath in audiopaths:\n",
        "  # load audio without reverb \n",
        "  audio, audio_sample_rate = librosa.load(audiopath, sr=sample_rate)\n",
        "  #print(f\"audio.shape[0]: {audio.shape[0]}\")\n",
        "\n",
        "  # collect alls audio without reverb in a list\n",
        "  audios.append(audio)\n",
        "\n",
        "  # split audio without reverb into chunks of length n\n",
        "  audio_chunks_1D = []\n",
        "  for j in range(0, audio.shape[0] - n, n):\n",
        "    # a single audio chunk without reverb of length n\n",
        "    audio_chunk = audio[j:j + n]\n",
        "    \n",
        "    # collect audio chunks without reverb in a list\n",
        "    audio_chunks_1D.append(audio_chunk)\n",
        "  \n",
        "  # collect audio chunks without reverb in a 2D list\n",
        "  audio_chunks_2D.append(audio_chunks_1D)\n",
        "\n",
        "\n",
        "# we convolve all audios with all IRs\n",
        "# counter for irpaths\n",
        "iridx = 0\n",
        "for irpath in irpaths:\n",
        "  # load IR functions\n",
        "  #ir_raw, _ = my_get_ir_sample(irpath, resample=sample_rate)\n",
        "  ir, IR_sample_rate = librosa.load(irpath, sr=sample_rate)\n",
        "  \n",
        "  # shorten all IR functions to the same length\n",
        "  #ir = ir[:, 0:int(sample_rate*2.0)]\n",
        "      \n",
        "  # collect IRs in a list\n",
        "  irs.append(ir)\n",
        "    \n",
        "  # loop over all audios without reverb\n",
        "  # counter for audios\n",
        "  audioidx = 0\n",
        "  for audio in audios:\n",
        "\n",
        "    try:\n",
        "    \n",
        "      # convole audio without reverb with IRs to obtain audio with reverb\n",
        "      #reverb_audio = my_convolve(ir, audio)\n",
        "      reverb_audio = my_conv(audio, ir)\n",
        "      #print(f\"reverb_audio.shape[1]: {reverb_audio.shape[1]}\")\n",
        "      \n",
        "      # collect audios with reverb in a list\n",
        "      reverb_audios.append(reverb_audio)\n",
        "\n",
        "      # counter for chunks\n",
        "      chunkidx = 0\n",
        "      # split audio with reverb into chunks of length n\n",
        "      for j in range(0, reverb_audio.shape[0] - n, n):\n",
        "\n",
        "        # a single audio chunk with reverb of length n\n",
        "        reverb_audio_chunk = reverb_audio[j:j + n]\n",
        "        \n",
        "        # collect audio chunks with reverb in a list\n",
        "        reverb_audio_chunks.append(reverb_audio_chunk)\n",
        "\n",
        "        # collect audio chunks without reverb in a list\n",
        "        audio_chunks.append(audio_chunks_2D[audioidx][chunkidx])\n",
        "        \n",
        "        # collect corresponding labels in a list\n",
        "        all_labels.append(unique_labels[iridx])\n",
        "        \n",
        "        # collect tuples audio chunks with reverb and labels in a list\n",
        "        labeled_reverb_audio_chunk = (reverb_audio_chunk, unique_labels[iridx])\n",
        "        labeled_reverb_audio_chunks.append(labeled_reverb_audio_chunk)\n",
        "        chunkidx = chunkidx + 1\n",
        "\n",
        "    except: print(\"Failed to convert audio. Signal shorter than IR (?)\")\n",
        "    \n",
        "    # increase counter for audios\n",
        "    audioidx = audioidx +1\n",
        "  \n",
        "  # increase counter for irpaths\n",
        "  iridx = iridx + 1\n",
        "\n",
        "# convert list to numpy arrays\n",
        "features = np.array(reverb_audio_chunks)\n",
        "targets = np.array(audio_chunks)\n",
        "labels = np.array(all_labels)\n",
        "print(f\"features.shape: {features.shape}\")\n",
        "print(f\"targets.shape: {targets.shape}\")\n",
        "print(f\"labels.shape: {labels.shape}\")\n",
        "\n",
        "# determine size of dataset\n",
        "dataset_size = labels.shape[0]\n",
        "\n",
        "# convert numpy arrays to tensorflow datasets\n",
        "# we can generate two different datasets:\n",
        "#   one with the labels as the targets\n",
        "#   and one with the sound without reverb as targets\n",
        "label_dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "target_dataset = tf.data.Dataset.from_tensor_slices((features, targets))\n",
        "\n",
        "# put it all in one dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features, labels, targets))\n",
        "\n",
        "# save the data as tensorflow dataset\n",
        "path = '/content/drive/My Drive/dsr_project/data/HarvardWordList/'\n",
        "tf.data.experimental.save(dataset, path + \"tf_3_randomIRFs_dataset\")"
      ],
      "metadata": {
        "id": "KErcslplB-WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2nSh-eBHULsi"
      },
      "outputs": [],
      "source": [
        "import tensorflow_datasets as tfds\n",
        "\n",
        "# load data\n",
        "# path of the tensorflow dataset \n",
        "path = '/content/drive/My Drive/dsr_project/data/HarvardWordList/'\n",
        "\n",
        "# load the tensorflow dataset\n",
        "dataset = tf.data.experimental.load(path + \"tf_3_randomIRFs_dataset\")\n",
        "\n",
        "# determine size of the dataset\n",
        "dataset_size = sum(1 for _ in dataset)\n",
        "print(f\"dataset_size: {dataset_size}\")\n",
        "\n",
        "def lambda_1(features, labels, targets):\n",
        "  return (features, labels)\n",
        "\n",
        "def lambda_2(features, labels, targets):\n",
        "  return (features, targets)\n",
        "\n",
        "def lambda_3(features, labels, targets):\n",
        "  return labels\n",
        "\n",
        "# obtain the datasets containing only labels or targets\n",
        "label_dataset = dataset.map(lambda features, labels, targets: lambda_1(features, labels, targets))\n",
        "target_dataset = dataset.map(lambda features, labels, targets: lambda_2(features, labels, targets))\n",
        "\n",
        "# obtain all labels\n",
        "all_labels = dataset.map(lambda features, labels, targets: lambda_3(features, labels, targets))\n",
        "\n",
        "# transform all_labels dataset to numpy array\n",
        "all_labels_np = []\n",
        "for label in all_labels:\n",
        "  all_labels_np.append(tfds.as_numpy(label))\n",
        "\n",
        "all_labels_np = np.array(all_labels_np)\n",
        "\n",
        "# determine unique labels\n",
        "unique_labels, counts = np.unique(all_labels_np, return_counts=True)\n",
        "print(f\"unique_labels: {unique_labels}\")\n",
        "#print(counts)\n",
        "\n",
        "# determine number of unique labels\n",
        "nr_of_unique_labels = unique_labels.shape[0]\n",
        "print(f\"nr_of_unique_labels: {nr_of_unique_labels}\")\n",
        "\n",
        "# shuffle the dataset before splitting in train and validate!\n",
        "label_dataset = label_dataset.shuffle(dataset_size)\n",
        "\n",
        "# split dataset in train and validate\n",
        "train_fraction = 0.8\n",
        "validate_dataset_size = int(dataset_size * (1.0-train_fraction)) # 20 percent of dataset_size\n",
        "train_dataset = label_dataset.skip(validate_dataset_size)\n",
        "validate_dataset = label_dataset.take(validate_dataset_size)"
      ]
    }
  ]
}